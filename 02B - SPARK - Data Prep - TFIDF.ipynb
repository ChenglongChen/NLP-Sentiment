{"nbformat_minor": 0, "cells": [{"execution_count": 47, "cell_type": "code", "source": "\"\"\" Pipeline for feature selection and classification\nUsing:\n\nhttps://spark.apache.org/docs/1.5.2/ml-features.html\nhttps://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html\nhttp://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionModel\nhttp://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html#sec:querydocweighting\n\nAttempting to replicate: \n\nclass sklearn.feature_extraction.text.TfidfVectorizer(input='content', encoding='utf-8',\ndecode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, \ntokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \nngram_range=(1, 3), max_df=1.0, min_df=1, max_features=40000, vocabulary=None, \nbinary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, \nsmooth_idf=True, sublinear_tf=True)\n\nI think only sublinear_tf and ngram_range need to be modified\n\n\"\"\"\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.sql.functions import col, udf\nfrom itertools import chain\nfrom pyspark.sql.types import ArrayType, StringType\nimport numpy as np\n\nnumfeat = 40000\n\n# 1. Feature-extraction\ndef concat(type):\n    \"\"\" UDF to concatenate lists across columns to create\n    an n-gram range. To reproduce ngram_range=(1,3) from sklearn\n    \"\"\"\n    def concat_(*args):\n        return list(chain(*args))\n    return udf(concat_, ArrayType(type))                   \nconcat_string_arrays = concat(StringType())\n\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"sentiment_idx\")\ntokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\nbiGram = NGram(inputCol = \"words\", n=2, outputCol = \"2gram\")\ntriGram = NGram(inputCol = \"words\", n=3, outputCol = \"3gram\")\nhashingtf  = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=numfeat)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\n\"\"\"\n# Apply sub-linear tf scaling!!!\nReplace tf with 1 + log(tf) like so:\n\n        if self.sublinear_tf:\n            np.log(X.data, X.data)\n            X.data += 1\n\n# Something like:\nsub_lin_tf = hashed_train.withColumn('lograwFeatures', np.log(hashed_train.rawFeatures)+1)\n\"\"\"\n\n#######\n# Train\n#######\nindexerModel = indexer.fit(trainingData)\ntrainingDataIx = indexerModel.transform(trainingData)\ntokenized_train = tokenizer.transform(trainingDataIx)\n\nbiGram_train = biGram.transform(tokenized_train)\ntriGram_train = triGram.transform(biGram_train)\nngrammed_train = triGram_train.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_train = hashingtf.transform(ngrammed_train)\n\n\n\nidfModel = idf.fit(hashed_train)\nidf_train = idfModel.transform(hashed_train)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 60, "cell_type": "code", "source": "# Apply sub-linear?\n\n#idf_train.first()['rawFeatures']\n\n#sub_lin_tf = hashed_train.withColumn('lograwFeatures', hashed_train.rawFeatures+1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "SparseVector(40000, {277: 1.0, 1056: 1.0, 1128: 1.0, 2165: 1.0, 3370: 1.0, 3371: 1.0, 3500: 1.0, 3707: 1.0, 4051: 1.0, 4486: 1.0, 4668: 1.0, 4846: 1.0, 5627: 1.0, 5646: 1.0, 6272: 1.0, 6944: 1.0, 7183: 1.0, 8088: 1.0, 9207: 1.0, 9541: 1.0, 9581: 1.0, 10564: 1.0, 11966: 1.0, 12365: 1.0, 12620: 1.0, 12709: 1.0, 13250: 1.0, 14081: 1.0, 15914: 1.0, 16727: 2.0, 16890: 1.0, 16897: 1.0, 16956: 1.0, 20127: 1.0, 20883: 1.0, 21925: 1.0, 22837: 1.0, 23051: 1.0, 23444: 1.0, 23895: 1.0, 23967: 1.0, 24280: 1.0, 24738: 1.0, 25310: 1.0, 25969: 1.0, 26033: 1.0, 26302: 1.0, 27086: 1.0, 29182: 1.0, 29509: 1.0, 29593: 1.0, 31576: 1.0, 32107: 1.0, 32586: 1.0, 34076: 1.0, 34380: 1.0, 34466: 1.0, 34619: 1.0, 35240: 1.0, 35615: 1.0, 36103: 2.0, 36715: 1.0, 37023: 1.0, 37075: 1.0, 37368: 1.0, 38027: 1.0, 38092: 1.0, 38621: 1.0, 39070: 1.0, 39352: 1.0})"}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "######\n# Test\n######\ntestDataIx = indexerModel.transform(testData)\ntokenized_test = tokenizer.transform(testDataIx)\n\nbiGram_test = biGram.transform(tokenized_test)\ntriGram_test = triGram.transform(biGram_test)\nngrammed_test = triGram_test.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_test = hashingtf.transform(ngrammed_test)\nidf_test = idfModel.transform(hashed_test)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 41, "cell_type": "code", "source": "# 2A. Classifier (Logistic Regression)\nclassi = LogisticRegression(labelCol=\"sentiment_idx\", featuresCol=\"features\")\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # 76.77 success rate\n\n\"\"\"\nStandard: 76.77 success rate\nWith ngrams(1,3): 88.17 success rate\nWith ngrams + sublineartf: ?\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "88.17 success rate\n'\\nStandard: 76.77 success rate\\nWith ngrams: ?\\nWith ngrams + sublineartf: ?\\n'\n/usr/hdp/current/spark-client/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n  warnings.warn(\"weights is deprecated. Use coefficients instead.\")"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# 2C. Classifier (GBTClassifier)\nclassi = GBTClassifier(labelCol=\"sentiment_idx\", featuresCol=\"features\", numClasses=2)\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # ? success rate", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 61, "cell_type": "code", "source": "# 3. Evaluation\npred.select(col('prediction'),col('sentiment_idx')).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-------------+\n|prediction|sentiment_idx|\n+----------+-------------+\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       0.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       0.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n+----------+-------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}