{"nbformat_minor": 0, "cells": [{"source": "## Upload Review Data using AzureML\n\nCreate a batch file and execute:\n    \n```\ncd \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\"\nAzCopy /Source:C:\\_ilia_share\\amazon_prod_reviews_clean\\raw /Dest:https://ikcentralusstore.blob.core.windows.net/amazonrev /DestKey:dLR5lH2QN/ejGmyD61nQoh7Cc2DW8jIKhR5n5uvGu8+H3Qem4J0XzWG1/7XtBxmVlWr+y/GNRlwX4Km5YU68sg== /Pattern:\"aggressive_dedup.json\"\npause\n```", "cell_type": "markdown", "metadata": {}}, {"source": "## Load Review Data (from Blob)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Idea courtesy of Thomas D.\nimport time\nSTIME = { \"start\" : time.time() }\n\ndef tic():\n    STIME[\"start\"] = time.time()\n\ndef toc():\n    elapsed = time.time() - STIME[\"start\"]\n    print(\"%.2f seconds elasped\" % elapsed)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>31</td><td>application_1469453428769_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ikclus.ftd4jbtqjxzuhd0uvhsmx0be3e.gx.internal.cloudapp.net:8088/proxy/application_1469453428769_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.10:30060/node/containerlogs/container_e04_1469453428769_0009_01_000001/spark\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "Creating HiveContext as 'sqlContext'\nSparkContext and HiveContext created. Executing user code ...\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# paths\nblob = \"wasb://amazonrev@ikcentralusstore.blob.core.windows.net\"\njson_dta = blob + \"/aggressive_dedup.json\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# load data\njsonFile = sqlContext.read.json(json_dta)\njsonFile.registerTempTable(\"reviews\")\n\nprint(type(jsonFile)) #  <class 'pyspark.sql.dataframe.DataFrame'>\njsonFile.show(5)\n\n# Note: also load the IMDB data at some point\n# ...", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|      asin|helpful|overall|          reviewText| reviewTime|          reviewerID|   reviewerName|             summary|unixReviewTime|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|B003UYU16G| [0, 0]|    5.0|It is and does ex...|11 21, 2012|A00000262KYZUE4J5...| Steven N Elich|Does what it's su...|    1353456000|\n|B005FYPK9C| [0, 0]|    5.0|I was sketchy at ...| 01 8, 2013|A000008615DZQRRI9...|      mj waldon|           great buy|    1357603200|\n|B000VEBG9Y| [0, 0]|    3.0|Very mobile produ...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great product but...|    1395619200|\n|B001EJMS6K| [0, 0]|    4.0|Easy to use a mob...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great inexpensive...|    1395619200|\n|B003XJCNVO| [0, 0]|    4.0|Love this feeder....|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great feeder. Wou...|    1395619200|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "## Examine some of the reviews", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "%%sql \nSELECT overall, reviewText\nFROM reviews\nLIMIT 10", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "%%sql \nSELECT overall, COUNT(overall) as freq\nFROM reviews\nGROUP BY overall\nORDER by -freq", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Create a dataframe of our reviews\n# To analyse class imbalance\nreviews =  sqlContext.sql(\"SELECT \" + \n                          \"CASE WHEN overall < 3 THEN 'low' \" +\n                          \"WHEN overall > 3 THEN 'high' ELSE 'mid' END as label, \" + \n                          \"reviewText as sentences \" + \n                          \"FROM reviews\")\n\ntally = reviews.groupBy(\"label\").count()\ntally.show()\n\n#mid| 7,039,272\n#low|10,963,811\n#high|64,453,794", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------+\n|label|   count|\n+-----+--------+\n|  mid| 7039272|\n|  low|10963811|\n| high|64453794|\n+-----+--------+"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Let's look at some reviews to see how clean they are\n# there seems to be lots of html formatting\nfor c,r in enumerate(reviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Some very basic cleaning\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType, DoubleType \nfrom bs4 import BeautifulSoup\n\ndef cleanerHTML(line):\n    # html formatting\n    html_clean = BeautifulSoup(line, \"lxml\").get_text().lower()\n    # remove any double spaces, line-breaks, etc.\n    return \" \".join(html_clean.split())\n\ndef labelForResults(s):\n    # string label to numeric\n    if s == 'low':\n        return 0.0\n    elif s == 'high':\n        return 1.0\n    else:\n        return -1.0\n        \ncleaner = UserDefinedFunction(cleanerHTML, StringType())\nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\ncleanedReviews = reviews.select(reviews.label,\n                                label(reviews.label).alias('sentiment'), \n                                cleaner(reviews.sentences).alias('sentences'))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# A bit cleaner ...\nfor c,r in enumerate(cleanedReviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n#cleanedReviews.show()\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Equalise classes \nneg_rev = cleanedReviews.filter(\"sentiment = 0.0\")\npos_rev = cleanedReviews.filter(\"sentiment = 1.0\").limit(neg_rev.count())\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Save data\nallData = pos_rev.unionAll(neg_rev)\nprint(allData.count()) # 21,927,622 ( = 10,963,811 * 2)\n\nallDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData.write.json(allDataLoc)\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load Clean Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "allDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData = sqlContext.read.json(allDataLoc)\n\ndata_count = allData.count()\nprint(data_count)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "21927622"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Take 100,000\nsub_sample = 1000000\nsub_sample_ratio = float(sub_sample)/float(data_count)\n\nprint(sub_sample_ratio)\nprint(type(allData))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.00456045803781\n<class 'pyspark.sql.dataframe.DataFrame'>"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# sub_sample -> sample(boolean withReplacement, double fraction, long seed)\nallData = allData.sample(False, sub_sample_ratio, 12345)\n\n# split intro training and test (50%, 50%)\ntrainingData, testData = allData.randomSplit([0.5, 0.5])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": "trainingDataLoc = blob + \"/training_1mill.json\"\ntestDataLoc = blob + \"/testing_1mill.json\"", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 33, "cell_type": "code", "source": "# Save\n#trainingData.write.mode(SaveMode.Overwrite).json(trainingDataLoc)\n#testData.write.mode(SaveMode.Overwrite).json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "# Load\ntrainingData = sqlContext.read.json(trainingDataLoc)\ntestData = sqlContext.read.json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "trainingData.cache()\ntestData.cache()\n\nprint(trainingData.count())\nprint(testData.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "500349\n499826"}], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "trainingData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|!i recommend this...|      1.0|\n| high|\" duty, honor, co...|      1.0|\n| high|\" ok let first st...|      1.0|\n| high|\"a deadly justice...|      1.0|\n| high|\"a dirty job\" is ...|      1.0|\n| high|\"a man of god\" is...|      1.0|\n| high|\"a practical book...|      1.0|\n| high|\"a widow's story\"...|      1.0|\n| high|\"abraham's burden...|      1.0|\n| high|\"always said if i...|      1.0|\n| high|\"american fool\" b...|      1.0|\n| high|\"antsy does time\"...|      1.0|\n| high|\"anyone who leads...|      1.0|\n| high|\"athlete/warrior\"...|      1.0|\n| high|\"better living th...|      1.0|\n| high|\"bob cornuke writ...|      1.0|\n| high|\"changing seasons...|      1.0|\n| high|\"city of angels: ...|      1.0|\n| high|\"cleopatra\" was t...|      1.0|\n| high|\"courage\" by dais...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 37, "cell_type": "code", "source": "testData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|\" to sheldon and ...|      1.0|\n| high|\"...those men who...|      1.0|\n| high|\"a prayer for the...|      1.0|\n| high|\"a seacat's love\"...|      1.0|\n| high|\"amazing product ...|      1.0|\n| high|\"aves - the age o...|      1.0|\n| high|\"butera does it a...|      1.0|\n| high|\"c'era una volta ...|      1.0|\n| high|\"circle william\" ...|      1.0|\n| high|\"crush proof\" is ...|      1.0|\n| high|\"die unendliche g...|      1.0|\n| high|\"don't let fear h...|      1.0|\n| high|\"du hast\" means y...|      1.0|\n| high|\"feedback\" is an ...|      1.0|\n| high|\"fling\" is one of...|      1.0|\n| high|\"forgiving maximo...|      1.0|\n| high|\"gangster governm...|      1.0|\n| high|\"gone girl\" just ...|      1.0|\n| high|\"good parents bad...|      1.0|\n| high|\"great price with...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}