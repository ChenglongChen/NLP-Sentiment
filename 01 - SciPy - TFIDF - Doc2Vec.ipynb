{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import, clean and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw/aggressive_dedup.json\n",
      "54.28077760338783  GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# Should have 82.83 million reviews\n",
    "filename = \"raw/aggressive_dedup.json\"\n",
    "print(filename)\n",
    "print(os.path.getsize(filename)/(1024*1024*1024), \" GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n",
      "It is and does exactly what the description said it would be and would do. Couldn't be happier with it.\n",
      "Clean:\n",
      "it is and does exactly what the description said it would be and would do . couldn ' t be happier with it .\n",
      " 5.0\n",
      "Raw:\n",
      "I was sketchy at first about these but once you wear them for a couple hours they break in they fit good on my board an have little wear from skating in them. They are a little heavy but won't get eaten up as bad by your grip tape like poser dc shoes.\n",
      "Clean:\n",
      "i was sketchy at first about these but once you wear them for a couple hours they break in they fit good on my board an have little wear from skating in them . they are a little heavy but won ' t get eaten up as bad by your grip tape like poser dc shoes .\n",
      " 5.0\n",
      "Raw:\n",
      "Very mobile product. Efficient. Easy to use; however product needs a varmint guard. Critters are able to gorge themselves without a guard.\n",
      "Clean:\n",
      "very mobile product . efficient . easy to use ; however product needs a varmint guard . critters are able to gorge themselves without a guard .\n",
      " 3.0\n",
      "Raw:\n",
      "Easy to use a mobile. If you're taller than 4ft, be ready to tuck your legs behind you as you hang and pull.\n",
      "Clean:\n",
      "easy to use a mobile . if you ' re taller than 4ft , be ready to tuck your legs behind you as you hang and pull .\n",
      " 4.0\n",
      "Raw:\n",
      "Love this feeder. Heavy duty & capacity. Best feature is the large varmint guard. Definitely use a small lock or securing device on the battery housing latch. I gave 4 stars because several bolts were missing. Check contents b4 beginning.\n",
      "Clean:\n",
      "love this feeder . heavy duty & capacity . best feature is the large varmint guard . definitely use a small lock or securing device on the battery housing latch . i gave 4 stars because several bolts were missing . check contents b4 beginning .\n",
      " 4.0\n",
      "Raw:\n",
      "Solid, stable mount. Holds iPhone with phone protector well. I have not however used the dash mount part of this product (only windshield).\n",
      "Clean:\n",
      "solid , stable mount . holds iphone with phone protector well . i have not however used the dash mount part of this product ( only windshield ) .\n",
      " 4.0\n",
      "Raw:\n",
      "I bought this pepper because I wanted a lot of cayenne powder I mean a lot. I drink shots of this powder daily and I do like it but I'm not sure if it's just me but it does not seem to strong I sweat for a minute but I feel like it could be stronger I even touched my eyes to see if it would hurt still not feeling much pain. Im one in 7 billion so do forget my review since I'm more hardcore.\n",
      "Clean:\n",
      "i bought this pepper because i wanted a lot of cayenne powder i mean a lot . i drink shots of this powder daily and i do like it but i ' m not sure if it ' s just me but it does not seem to strong i sweat for a minute but i feel like it could be stronger i even touched my eyes to see if it would hurt still not feeling much pain . im one in 7 billion so do forget my review since i ' m more hardcore .\n",
      " 5.0\n"
     ]
    }
   ],
   "source": [
    "def line_feeder(fname):\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            js_out = json.loads(line) \n",
    "            yield js_out   \n",
    "        \n",
    "def clean_review(review):\n",
    "    temp = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    punctuation = \"\"\"'.,?!:;(){}[]\"\"\"\n",
    "    for char in punctuation:\n",
    "        temp = temp.replace(char, ' ' + char + ' ')\n",
    "    words = \" \".join(temp.lower().split()) + \"\\n\"\n",
    "    return words\n",
    "\n",
    "def example(cut=5):\n",
    "    for c,x in enumerate(line_feeder(filename)):\n",
    "        rev, rating = clean_review(x[\"reviewText\"]), x[\"overall\"]\n",
    "        print(\"Raw:\")\n",
    "        print(x[\"reviewText\"]), x[\"overall\"]\n",
    "        print(\"Clean:\")\n",
    "        print(rev, rating)          \n",
    "        if c > cut:\n",
    "            return\n",
    "        \n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "good_rev = []\n",
    "bad_rev = []\n",
    "neut_rev = []\n",
    "error_rev = []\n",
    "\n",
    "gr = open('good_reviews.txt', 'w', encoding='utf-8')\n",
    "br = open('bad_reviews.txt', 'w', encoding='utf-8')\n",
    "nt = open('neutral_reviews.txt', 'w', encoding='utf-8')\n",
    "er = open('error_reviews.txt', 'w', encoding='utf-8')\n",
    "\n",
    "chunks = 0\n",
    "stime = time.time()\n",
    "for x in line_feeder(filename):\n",
    "    \n",
    "    chunks += 1\n",
    "    rev, rating = clean_review(x[\"reviewText\"]), x[\"overall\"]\n",
    "    \n",
    "    if not len(rev) > 2:\n",
    "        # Fewer than 3 characters not meangingful\n",
    "        error_rev.append(rev)\n",
    "    else:\n",
    "        # Review long enough to consider\n",
    "        if rating in [4,5]:\n",
    "            good_rev.append(rev)\n",
    "        elif rating in [1,2]:\n",
    "            bad_rev.append(rev)\n",
    "        else:\n",
    "            neut_rev.append(rev)\n",
    "            \n",
    "    # Chunk every N=1000*000 reviews\n",
    "    # Limited by IO, disk = 96%\n",
    "    # Takes 305 seconds for 1mill, so around 420 minutes = 7 hours\n",
    "    if chunks % (1000*1000) == 0:\n",
    "        print(\"Processed: %d records\" % chunks)\n",
    "        print(\"Elapsed: %.2f\" % (time.time() - stime))\n",
    "\n",
    "        gr.writelines(good_rev)\n",
    "        br.writelines(bad_rev)\n",
    "        nt.writelines(neut_rev)\n",
    "        er.writelines(error_rev)\n",
    "\n",
    "        good_rev = []\n",
    "        bad_rev = []\n",
    "        neut_rev = []\n",
    "        error_rev = []\n",
    "            \n",
    "# Any remaining\n",
    "gr.writelines(good_rev)\n",
    "gr.close()\n",
    "br.writelines(bad_rev)\n",
    "br.close()\n",
    "nt.writelines(neut_rev)\n",
    "nt.close()\n",
    "er.writelines(error_rev)\n",
    "er.close()\n",
    "\n",
    "del good_rev\n",
    "del bad_rev\n",
    "del neut_rev\n",
    "del error_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check sizes\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "# Should add up:\n",
    "# print(\"Raw contains %d lines\" % file_len(filename))\n",
    "\n",
    "# We have 64,439,865 good reviews\n",
    "# print(\"Good contains %d lines\" % file_len('good_reviews.txt'))\n",
    "# We have 10,961,504 bad reviews\n",
    "# print(\"Bad contains %d lines\" % file_len('bad_reviews.txt'))\n",
    "\n",
    "\n",
    "# print(\"Neutral contains %d lines\" % file_len('neutral_reviews.txt'))\n",
    "# print(\"Short contains %d lines\" % file_len('error_reviews.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1 mill \n",
    "_SAMPLE_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split data into train and test (also use subsample):\n",
    "import random\n",
    "\n",
    "def train_test_split(train_ratio=0.5):\n",
    "    # Train -> true\n",
    "    return random.uniform(0,1) <= train_ratio\n",
    "\n",
    "def line_feeder(fname, cutoff):\n",
    "    i = 0\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            i+=1\n",
    "            if i == cutoff:\n",
    "                break\n",
    "            \n",
    "def split_data(dataname, sample_size, train_ratio):\n",
    "    with open('train_' + dataname, 'w', encoding='utf-8') as tr:\n",
    "        with open('test_' + dataname, 'w', encoding='utf-8') as te:\n",
    "            for line in line_feeder(dataname, sample_size):\n",
    "                if train_test_split(0.5):\n",
    "                    tr.write(line)\n",
    "                else:\n",
    "                    te.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "split_data(dataname = 'good_reviews.txt', sample_size = _SAMPLE_SIZE, train_ratio = 0.5)\n",
    "split_data(dataname = 'bad_reviews.txt', sample_size = _SAMPLE_SIZE, train_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sources = {'test_bad_reviews.txt':'TE_B',\n",
    "           'test_good_reviews.txt':'TE_G',\n",
    "           'train_bad_reviews.txt':'TR_B',\n",
    "           'train_good_reviews.txt':'TR_G'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TFIDF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "def file_to_list(fname):\n",
    "    with utils.smart_open(fname) as f:\n",
    "        for rev in f:\n",
    "            yield rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Training Data\n",
    "clean_train_reviews = []\n",
    "train_labels = []\n",
    "for f in ['train_good_reviews.txt',\n",
    "          'train_bad_reviews.txt']:\n",
    "    for review in file_to_list(f):\n",
    "        clean_train_reviews.append(utils.to_unicode(review))\n",
    "        if \"good\" in f:\n",
    "            train_labels.append(1)\n",
    "        elif \"bad\" in f:\n",
    "            train_labels.append(0)\n",
    "        else:\n",
    "            raise Exception\n",
    "        \n",
    "print(\"Sample review: %s\" % clean_train_reviews[0])\n",
    "print(\"Vectorising ... %d reviews\" % len(clean_train_reviews))\n",
    "\n",
    "## Defaults:\n",
    "#TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict',\n",
    "#                strip_accents=None, lowercase=True, preprocessor=None,\n",
    "#                tokenizer=None, analyzer='word', stop_words=None,\n",
    "#                token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1),\n",
    "#                max_df=1.0, min_df=1, max_features=None, vocabulary=None,\n",
    "#                binary=False, dtype=<class 'numpy.int64'>, norm='l2',\n",
    "#                use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "\n",
    "# 40k, tri-grams, sublinear\n",
    "vectorizer = TfidfVectorizer(max_features = 40000, ngram_range = (1, 3), sublinear_tf = True)\n",
    "\n",
    "# Learn vocabulary and idf, return term-document matrix\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "del clean_train_reviews\n",
    "\n",
    "# Training Model\n",
    "\n",
    "## Defaults used\n",
    "#LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "#                   intercept_scaling=1, class_weight=None, random_state=None,\n",
    "#                   solver='liblinear', max_iter=100, multi_class='ovr',\n",
    "#                   verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "#GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100,\n",
    "#                           subsample=1.0, min_samples_split=2, min_samples_leaf=1,\n",
    "#                           min_weight_fraction_leaf=0.0, max_depth=3, init=None,\n",
    "#                           random_state=None, max_features=None, verbose=0, \n",
    "#                           max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "\n",
    "# l2, max_iter = 100\n",
    "classifier_tfidf = LogisticRegression()\n",
    "\n",
    "classifier_tfidf.fit(train_data_features, train_labels)\n",
    "\n",
    "del train_data_features\n",
    "del train_labels\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing Data\n",
    "clean_test_reviews = []\n",
    "test_labels = []\n",
    "for f in ['test_good_reviews.txt',\n",
    "          'test_bad_reviews.txt']:\n",
    "    for review in file_to_list(f):\n",
    "        clean_test_reviews.append(utils.to_unicode(review))\n",
    "        if \"good\" in f:\n",
    "            test_labels.append(1)\n",
    "        elif \"bad\" in f:\n",
    "            test_labels.append(0)\n",
    "        else:\n",
    "            raise Exception\n",
    "                   \n",
    "print(\"Sample review: %s\" % clean_test_reviews[0])\n",
    "print(\"Vectorising ... %d reviews\" % len(clean_test_reviews))  \n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "\n",
    "del clean_test_reviews\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run classifier\n",
    "classifier_tfidf.score(test_data_features.toarray(), test_labels) \n",
    "\n",
    "# 50k gives 0.915\n",
    "# 500k gives 0.928\n",
    "# 1mill gives 0.931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tricky_sentences = [\n",
    "    \"Most movies are rubbish, however this one was good\",\n",
    "    \"This is a product you would love to hate\",\n",
    "    \"Rubbish from start to finish\",\n",
    "    \"I was so happy when this ended\",\n",
    "    \"Very good\",\n",
    "    \"Horrible\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some tests ...\n",
    "def tfidf_sample_sentiment(mystr):\n",
    "    assert isinstance(mystr, str)\n",
    "    test_data_feat = vectorizer.transform([mystr])\n",
    "    pred = classifier_tfidf.predict(test_data_feat)\n",
    "    return pred\n",
    "\n",
    "for x in tricky_sentences:\n",
    "    clean = clean_review(x)\n",
    "    print(clean, tfidf_sample_sentiment(clean))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del vectorizer\n",
    "del classifier_tfidf\n",
    "del test_data_features\n",
    "del test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Doc2Vec Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from random import shuffle\n",
    "import multiprocessing\n",
    "\n",
    "import gensim.models.doc2vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "print(cores)\n",
    "\n",
    "model_feat_size = 400\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "    \n",
    "    def extract_sentences(self):\n",
    "        self.sentences = []\n",
    "        for sr, pr in self.sources.items():\n",
    "            with utils.smart_open(sr) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(),\n",
    "                                                          [pr + '_%s' % item_no]))\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This will be a list of all sentences\n",
    "sentences = LabeledLineSentence(sources)\n",
    "sentences.extract_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mikov's params for IMDB](http://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/quoc-response.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t24)\n",
      "Doc2Vec(dbow,d100,n5,mc2,t24)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,t24)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Creating models using params from Mikolov above and here:\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "\n",
    "# Note window = 5 is both sides and approximates the above window = 10 (diff to Mikolov's scripts)\n",
    "\n",
    "single_models = [\n",
    "    # PV-DM with concatenation\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW\n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM with averaging\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores)\n",
    "]\n",
    "\n",
    "single_models[0].build_vocab(sentences.sentences)\n",
    "print(single_models[0])\n",
    "\n",
    "# Use learnt vocab for other models\n",
    "for mod in single_models[1:]:\n",
    "    mod.reset_from(single_models[0])\n",
    "    print(mod)\n",
    "\n",
    "nms = ['dm_concat', 'dbow', 'dm_averaging']\n",
    "models = OrderedDict((str(name), model) for model, name in zip(single_models, nms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('dm_concat', <gensim.models.doc2vec.Doc2Vec object at 0x0000019B4BF5B240>), ('dbow', <gensim.models.doc2vec.Doc2Vec object at 0x0000019B4BF5B278>), ('dm_averaging', <gensim.models.doc2vec.Doc2Vec object at 0x0000019B4BF5B2E8>), ('dbow_dmm', <gensim.test.test_doc2vec.ConcatenatedDoc2Vec object at 0x0000019B442C76A0>), ('dbow_dmc', <gensim.test.test_doc2vec.ConcatenatedDoc2Vec object at 0x0000019B442C75F8>)])\n"
     ]
    }
   ],
   "source": [
    "# Also use concatenated vectors from each model\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "models['dbow_dmm'] = ConcatenatedDoc2Vec([single_models[1], single_models[2]])\n",
    "models['dbow_dmc'] = ConcatenatedDoc2Vec([single_models[1], single_models[0]])\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: bigreviews_subsample_1mill_400d_dm_concat.d2v\n",
      "Training epoch: 0\n"
     ]
    }
   ],
   "source": [
    "# Train model:\n",
    "for nme, model in models.items():\n",
    "    \n",
    "    alpha, min_alpha, epochs = (0.025, 0.001, 20)\n",
    "    alpha_delta = (alpha - min_alpha) / epochs\n",
    "    \n",
    "    print(\"Training model: bigreviews_subsample_1mill_400d_%s.d2v\" % nme)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(\"Training epoch: %d\" % epoch)\n",
    "        \n",
    "        model.alpha, model.min_alpha = alpha, alpha\n",
    "        model.train(sentences.sentences_perm())\n",
    "        alpha -= alpha_delta\n",
    "        \n",
    "    print(\"Finished training model: %s\" % nme)\n",
    "    model.save(\"bigreviews_subsample_1mill_400d_%s.d2v\" % nme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Play around with the features of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load a model\n",
    "#model = Doc2Vec.load('bigreviews_subsample_1mill_1200d.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('spielberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar('refund')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_from_file(fname, line_no):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i == line_no:\n",
    "                return l\n",
    "            \n",
    "print(line_from_file('train_good_reviews.txt',0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.docvecs.most_similar('TR_G_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(line_from_file('test_good_reviews.txt', 347295))\n",
    "print(line_from_file('test_good_reviews.txt', 39105))\n",
    "print(line_from_file('train_good_reviews.txt', 10921))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infer a vector ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = Doc2Vec.load('bigreviews_subsample_1mill_1200d.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check sizes\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "# Dictionary of file-lengths\n",
    "file_lengths = {}\n",
    "for k,v in sources.items():\n",
    "    file_lengths[v] = file_len(k)\n",
    "\n",
    "file_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# Train vectors\n",
    "no_training = file_lengths['TR_G'] + file_lengths['TR_B']\n",
    "print(\"%d for training\" % no_training)\n",
    "\n",
    "train_arrays = numpy.zeros((no_training, model_feat_size))\n",
    "train_labels = numpy.concatenate((numpy.ones(file_lengths['TR_G']),\n",
    "                                  numpy.zeros(file_lengths['TR_B'])))\n",
    "\n",
    "for i in range(file_lengths['TR_G']):\n",
    "    train_arrays[i] = model.docvecs['TR_G_' + str(i)]\n",
    "    \n",
    "for i in range(file_lengths['TR_B']):\n",
    "    train_arrays[file_lengths['TR_G'] + i] = model.docvecs['TR_B_' + str(i)]\n",
    "    \n",
    "assert len(train_arrays) == len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.svm import LinearSVC\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = LogisticRegression()  # 0.84834202356611155\n",
    "#classifier = RandomForestClassifier\n",
    "#classifier = SGDClassifier(loss='log', penalty='l1')  # 0.84822804237300842\n",
    "#classifier = LinearSVC() # 0.84843300855358861\n",
    "#classifier = MLPClassifier(hidden_layer_sizes = (50,))\n",
    "\n",
    "classifier.fit(train_arrays, train_labels)\n",
    "\n",
    "del train_arrays\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test vectors\n",
    "no_testing = file_lengths['TE_G'] + file_lengths['TE_B']\n",
    "print(\"%d for testing\" % no_testing)\n",
    "\n",
    "test_arrays = numpy.zeros((no_testing, model_feat_size))\n",
    "test_labels = numpy.concatenate((numpy.ones(file_lengths['TE_G']),\n",
    "                                  numpy.zeros(file_lengths['TE_B'])))\n",
    "\n",
    "for i in range(file_lengths['TE_G']):\n",
    "    test_arrays[i] = model.docvecs['TE_G_' + str(i)]\n",
    "    \n",
    "for i in range(file_lengths['TE_B']):\n",
    "    test_arrays[file_lengths['TE_G'] + i] = model.docvecs['TE_B_' + str(i)]\n",
    "\n",
    "assert len(test_arrays) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(test_arrays, test_labels)  # 0.85499592567226412"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
