{"nbformat_minor": 0, "cells": [{"source": "## Upload Review Data using AzureML\n\nCreate a batch file and execute:\n    \n```\ncd \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\"\nAzCopy /Source:C:\\_ilia_share\\amazon_prod_reviews_clean\\raw /Dest:https://ikcentralusstore.blob.core.windows.net/amazonrev /DestKey:dLR5lH2QN/ejGmyD61nQoh7Cc2DW8jIKhR5n5uvGu8+H3Qem4J0XzWG1/7XtBxmVlWr+y/GNRlwX4Km5YU68sg== /Pattern:\"aggressive_dedup.json\"\npause\n```", "cell_type": "markdown", "metadata": {}}, {"source": "## Load Review Data (from Blob)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Idea courtesy of Thomas D.\nimport time\nSTIME = { \"start\" : time.time() }\n\ndef tic():\n    STIME[\"start\"] = time.time()\n\ndef toc():\n    elapsed = time.time() - STIME[\"start\"]\n    print(\"%.2f seconds elasped\" % elapsed)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>31</td><td>application_1469453428769_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ikclus.ftd4jbtqjxzuhd0uvhsmx0be3e.gx.internal.cloudapp.net:8088/proxy/application_1469453428769_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.10:30060/node/containerlogs/container_e04_1469453428769_0009_01_000001/spark\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "Creating HiveContext as 'sqlContext'\nSparkContext and HiveContext created. Executing user code ...\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# paths\nblob = \"wasb://amazonrev@ikcentralusstore.blob.core.windows.net\"\njson_dta = blob + \"/aggressive_dedup.json\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# load data\njsonFile = sqlContext.read.json(json_dta)\njsonFile.registerTempTable(\"reviews\")\n\nprint(type(jsonFile)) #  <class 'pyspark.sql.dataframe.DataFrame'>\njsonFile.show(5)\n\n# Note: also load the IMDB data at some point\n# ...", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|      asin|helpful|overall|          reviewText| reviewTime|          reviewerID|   reviewerName|             summary|unixReviewTime|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|B003UYU16G| [0, 0]|    5.0|It is and does ex...|11 21, 2012|A00000262KYZUE4J5...| Steven N Elich|Does what it's su...|    1353456000|\n|B005FYPK9C| [0, 0]|    5.0|I was sketchy at ...| 01 8, 2013|A000008615DZQRRI9...|      mj waldon|           great buy|    1357603200|\n|B000VEBG9Y| [0, 0]|    3.0|Very mobile produ...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great product but...|    1395619200|\n|B001EJMS6K| [0, 0]|    4.0|Easy to use a mob...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great inexpensive...|    1395619200|\n|B003XJCNVO| [0, 0]|    4.0|Love this feeder....|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great feeder. Wou...|    1395619200|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "## Examine some of the reviews", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "%%sql \nSELECT overall, reviewText\nFROM reviews\nLIMIT 10", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "%%sql \nSELECT overall, COUNT(overall) as freq\nFROM reviews\nGROUP BY overall\nORDER by -freq", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Create a dataframe of our reviews\n# To analyse class imbalance\nreviews =  sqlContext.sql(\"SELECT \" + \n                          \"CASE WHEN overall < 3 THEN 'low' \" +\n                          \"WHEN overall > 3 THEN 'high' ELSE 'mid' END as label, \" + \n                          \"reviewText as sentences \" + \n                          \"FROM reviews\")\n\ntally = reviews.groupBy(\"label\").count()\ntally.show()\n\n#mid| 7,039,272\n#low|10,963,811\n#high|64,453,794", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------+\n|label|   count|\n+-----+--------+\n|  mid| 7039272|\n|  low|10963811|\n| high|64453794|\n+-----+--------+"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Let's look at some reviews to see how clean they are\n# there seems to be lots of html formatting\nfor c,r in enumerate(reviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Some very basic cleaning\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType, DoubleType \nfrom bs4 import BeautifulSoup\n\ndef cleanerHTML(line):\n    # html formatting\n    html_clean = BeautifulSoup(line, \"lxml\").get_text().lower()\n    # remove any double spaces, line-breaks, etc.\n    return \" \".join(html_clean.split())\n\ndef labelForResults(s):\n    # string label to numeric\n    if s == 'low':\n        return 0.0\n    elif s == 'high':\n        return 1.0\n    else:\n        return -1.0\n        \ncleaner = UserDefinedFunction(cleanerHTML, StringType())\nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\ncleanedReviews = reviews.select(reviews.label,\n                                label(reviews.label).alias('sentiment'), \n                                cleaner(reviews.sentences).alias('sentences'))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# A bit cleaner ...\nfor c,r in enumerate(cleanedReviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n#cleanedReviews.show()\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Equalise classes \nneg_rev = cleanedReviews.filter(\"sentiment = 0.0\")\npos_rev = cleanedReviews.filter(\"sentiment = 1.0\").limit(neg_rev.count())\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Save data\nallData = pos_rev.unionAll(neg_rev)\nprint(allData.count()) # 21,927,622 ( = 10,963,811 * 2)\n\nallDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData.write.json(allDataLoc)\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load Clean Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "allDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData = sqlContext.read.json(allDataLoc)\n\ndata_count = allData.count()\nprint(data_count)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "21927622"}], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Take 100,000\nsub_sample = 1000000\nsub_sample_ratio = float(sub_sample)/float(data_count)\n\nprint(sub_sample_ratio)\nprint(type(allData))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.00456045803781\n<class 'pyspark.sql.dataframe.DataFrame'>"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# sub_sample -> sample(boolean withReplacement, double fraction, long seed)\nallData = allData.sample(False, sub_sample_ratio, 12345)\n\n# split intro training and test (50%, 50%)\ntrainingData, testData = allData.randomSplit([0.5, 0.5])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": "trainingDataLoc = blob + \"/training_1mill.json\"\ntestDataLoc = blob + \"/testing_1mill.json\"", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 33, "cell_type": "code", "source": "# Save\n#trainingData.write.mode(SaveMode.Overwrite).json(trainingDataLoc)\n#testData.write.mode(SaveMode.Overwrite).json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "# Load\ntrainingData = sqlContext.read.json(trainingDataLoc)\ntestData = sqlContext.read.json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "trainingData.cache()\ntestData.cache()\n\nprint(trainingData.count())\nprint(testData.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "500349\n499826"}], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "trainingData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|!i recommend this...|      1.0|\n| high|\" duty, honor, co...|      1.0|\n| high|\" ok let first st...|      1.0|\n| high|\"a deadly justice...|      1.0|\n| high|\"a dirty job\" is ...|      1.0|\n| high|\"a man of god\" is...|      1.0|\n| high|\"a practical book...|      1.0|\n| high|\"a widow's story\"...|      1.0|\n| high|\"abraham's burden...|      1.0|\n| high|\"always said if i...|      1.0|\n| high|\"american fool\" b...|      1.0|\n| high|\"antsy does time\"...|      1.0|\n| high|\"anyone who leads...|      1.0|\n| high|\"athlete/warrior\"...|      1.0|\n| high|\"better living th...|      1.0|\n| high|\"bob cornuke writ...|      1.0|\n| high|\"changing seasons...|      1.0|\n| high|\"city of angels: ...|      1.0|\n| high|\"cleopatra\" was t...|      1.0|\n| high|\"courage\" by dais...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 37, "cell_type": "code", "source": "testData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|\" to sheldon and ...|      1.0|\n| high|\"...those men who...|      1.0|\n| high|\"a prayer for the...|      1.0|\n| high|\"a seacat's love\"...|      1.0|\n| high|\"amazing product ...|      1.0|\n| high|\"aves - the age o...|      1.0|\n| high|\"butera does it a...|      1.0|\n| high|\"c'era una volta ...|      1.0|\n| high|\"circle william\" ...|      1.0|\n| high|\"crush proof\" is ...|      1.0|\n| high|\"die unendliche g...|      1.0|\n| high|\"don't let fear h...|      1.0|\n| high|\"du hast\" means y...|      1.0|\n| high|\"feedback\" is an ...|      1.0|\n| high|\"fling\" is one of...|      1.0|\n| high|\"forgiving maximo...|      1.0|\n| high|\"gangster governm...|      1.0|\n| high|\"gone girl\" just ...|      1.0|\n| high|\"good parents bad...|      1.0|\n| high|\"great price with...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## 1. TFIDF", "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": "\"\"\" Pipeline for feature selection and classification\nUsing:\n\nhttps://spark.apache.org/docs/1.5.2/ml-features.html\nhttps://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html\nhttp://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionModel\nhttp://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html#sec:querydocweighting\n\nAttempting to replicate: \n\nclass sklearn.feature_extraction.text.TfidfVectorizer(input='content', encoding='utf-8',\ndecode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, \ntokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \nngram_range=(1, 3), max_df=1.0, min_df=1, max_features=40000, vocabulary=None, \nbinary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, \nsmooth_idf=True, sublinear_tf=True)\n\nI think only sublinear_tf and ngram_range need to be modified\n\n\"\"\"\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.sql.functions import col, udf\nfrom itertools import chain\nfrom pyspark.sql.types import ArrayType, StringType\nimport numpy as np\n\nnumfeat = 40000\n\n# 1. Feature-extraction\ndef concat(type):\n    \"\"\" UDF to concatenate lists across columns to create\n    an n-gram range. To reproduce ngram_range=(1,3) from sklearn\n    \"\"\"\n    def concat_(*args):\n        return list(chain(*args))\n    return udf(concat_, ArrayType(type))                   \nconcat_string_arrays = concat(StringType())\n\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"sentiment_idx\")\ntokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\nbiGram = NGram(inputCol = \"words\", n=2, outputCol = \"2gram\")\ntriGram = NGram(inputCol = \"words\", n=3, outputCol = \"3gram\")\nhashingtf  = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=numfeat)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\n\"\"\"\n# Apply sub-linear tf scaling!!!\nReplace tf with 1 + log(tf) like so:\n\n        if self.sublinear_tf:\n            np.log(X.data, X.data)\n            X.data += 1\n\n# Something like:\nsub_lin_tf = hashed_train.withColumn('lograwFeatures', np.log(hashed_train.rawFeatures)+1)\n\"\"\"\n\n#######\n# Train\n#######\nindexerModel = indexer.fit(trainingData)\ntrainingDataIx = indexerModel.transform(trainingData)\ntokenized_train = tokenizer.transform(trainingDataIx)\n\nbiGram_train = biGram.transform(tokenized_train)\ntriGram_train = triGram.transform(biGram_train)\nngrammed_train = triGram_train.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_train = hashingtf.transform(ngrammed_train)\n\n\n\nidfModel = idf.fit(hashed_train)\nidf_train = idfModel.transform(hashed_train)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 60, "cell_type": "code", "source": "# Apply sub-linear?\n\n#idf_train.first()['rawFeatures']\n\n#sub_lin_tf = hashed_train.withColumn('lograwFeatures', hashed_train.rawFeatures+1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "SparseVector(40000, {277: 1.0, 1056: 1.0, 1128: 1.0, 2165: 1.0, 3370: 1.0, 3371: 1.0, 3500: 1.0, 3707: 1.0, 4051: 1.0, 4486: 1.0, 4668: 1.0, 4846: 1.0, 5627: 1.0, 5646: 1.0, 6272: 1.0, 6944: 1.0, 7183: 1.0, 8088: 1.0, 9207: 1.0, 9541: 1.0, 9581: 1.0, 10564: 1.0, 11966: 1.0, 12365: 1.0, 12620: 1.0, 12709: 1.0, 13250: 1.0, 14081: 1.0, 15914: 1.0, 16727: 2.0, 16890: 1.0, 16897: 1.0, 16956: 1.0, 20127: 1.0, 20883: 1.0, 21925: 1.0, 22837: 1.0, 23051: 1.0, 23444: 1.0, 23895: 1.0, 23967: 1.0, 24280: 1.0, 24738: 1.0, 25310: 1.0, 25969: 1.0, 26033: 1.0, 26302: 1.0, 27086: 1.0, 29182: 1.0, 29509: 1.0, 29593: 1.0, 31576: 1.0, 32107: 1.0, 32586: 1.0, 34076: 1.0, 34380: 1.0, 34466: 1.0, 34619: 1.0, 35240: 1.0, 35615: 1.0, 36103: 2.0, 36715: 1.0, 37023: 1.0, 37075: 1.0, 37368: 1.0, 38027: 1.0, 38092: 1.0, 38621: 1.0, 39070: 1.0, 39352: 1.0})"}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "######\n# Test\n######\ntestDataIx = indexerModel.transform(testData)\ntokenized_test = tokenizer.transform(testDataIx)\n\nbiGram_test = biGram.transform(tokenized_test)\ntriGram_test = triGram.transform(biGram_test)\nngrammed_test = triGram_test.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_test = hashingtf.transform(ngrammed_test)\nidf_test = idfModel.transform(hashed_test)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 41, "cell_type": "code", "source": "# 2A. Classifier (Logistic Regression)\nclassi = LogisticRegression(labelCol=\"sentiment_idx\", featuresCol=\"features\")\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # 76.77 success rate\n\n\"\"\"\nStandard: 76.77 success rate\nWith ngrams(1,3): 88.17 success rate\nWith ngrams + sublineartf: ?\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "88.17 success rate\n'\\nStandard: 76.77 success rate\\nWith ngrams: ?\\nWith ngrams + sublineartf: ?\\n'\n/usr/hdp/current/spark-client/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n  warnings.warn(\"weights is deprecated. Use coefficients instead.\")"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# 2C. Classifier (GBTClassifier)\nclassi = GBTClassifier(labelCol=\"sentiment_idx\", featuresCol=\"features\", numClasses=2)\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # ? success rate", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 61, "cell_type": "code", "source": "# 3. Evaluation\npred.select(col('prediction'),col('sentiment_idx')).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-------------+\n|prediction|sentiment_idx|\n+----------+-------------+\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       0.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       0.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n|       1.0|          1.0|\n+----------+-------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {"501f6eb38c3e4cf7b1c4011e54a5459e": {"views": []}, "90d88c2f50434eef94677e195a52aa88": {"views": []}, "8f7fff41c9ef437eb6b44a2c25eea877": {"views": []}, "4c91597359ce4a71bd1fc61e964296fc": {"views": []}, "dc355393f9d1469aa5a8d1b341a35bea": {"views": []}, "de0caff9b14a43f9a83b9c7aac8adf77": {"views": []}, "858d21e26d4d4170870c73a705d04db5": {"views": []}, "35713830a9c647539358a61148f5745d": {"views": []}, "aac3ad11115c401f89d9b5fa4a0fe339": {"views": []}, "eb3ffc64aba54f24b8db7007923a7cdf": {"views": []}, "76ef604b1f564fff82e6a7dcb59a4ad9": {"views": []}, "4ec15aa1ed1a43ff9664b0a85967c05f": {"views": []}, "82fa6b554bd14b8ebac0c6d8e1662f92": {"views": []}, "05a845f3f2504b149f10311c23e1373f": {"views": []}, "cfb93643a40f42f598a549bf1f3f9de6": {"views": []}, "2722d3957f5d437ea36809dd693c8f67": {"views": []}, "cdae9136ae7c470692252a49de819469": {"views": []}, "4ce160f5ff5e4eb49b747af45ab57855": {"views": []}, "5caba7198bfe45cf984e9059106bb1ba": {"views": []}, "de0df90a954441c58f6ce13ba081d515": {"views": []}, "f1d4b6a3ec514d2ea6aa8688caf73bf8": {"views": []}, "70a836dde37d42bdbe372d9f003a6d57": {"views": []}, "cc31b9050ef34fd6af0a92071923c5a3": {"views": []}, "98bbc9e449ae4065871b25cb11baed4d": {"views": []}, "285b7e5c54794827b6fd6d891a51251a": {"views": []}, "ad5b933535a94a11a62282f7e8daf02e": {"views": []}, "4dc094ef612b45fe86a250b80cd7dea7": {"views": []}, "1883f443a1a9496db52e3a6d83684195": {"views": []}, "d897373cd2b94f14af6eab338d9f377c": {"views": [{"cell_index": 6}]}, "9bed5f7650d644b7ba9e7f81e170ee7d": {"views": []}, "b9b4b4d9cc7148a1a36a8b690b491b2b": {"views": []}, "bef790d6e4704d7a8f1acc6519057b01": {"views": []}, "a22ad8dfe634467b92fd83b8567f81d5": {"views": []}, "309a9497d3b4415ea2585ba11ffaacb3": {"views": []}, "2be4033b0e4c4f49973aa9dc391359d0": {"views": []}, "ae1a272c9c9e4a86a6c551b328402fc3": {"views": []}, "d4666b33e3fd4b44939f5237f556d3bc": {"views": []}, "305eca9f382a40fc85670b04fa81fe53": {"views": []}, "e8344c2474cd4d918e4ec73e635ea489": {"views": []}, "9caaf45a1d624d679019d9551f9c7d10": {"views": []}, "ddbb8e5587354f9bb3f8dfc09d88c07f": {"views": []}, "108fcb3b5e5b487d920cf5237112e27a": {"views": []}, "195fb149eaca4cd58a6859d6550701e8": {"views": []}, "045304c31db84ec99581cdd5874898df": {"views": []}, "fc4300ffabd94d1fafae6605611a8328": {"views": []}, "792d42f751c147b4a5c6392a7fa125a1": {"views": []}, "3149385249d4493c961e44362215fa08": {"views": []}, "f222eae22e3546d099c3902f7717bdd0": {"views": []}, "a9eddc791c49406daba49d64ebabe0a3": {"views": []}, "2cfb529980d24e63ad93f9e476d4fb1d": {"views": []}, "e9c6737edd0c44a0b21487f3ffea14a1": {"views": []}, "5f14c860269441b7adc9cec699afad16": {"views": []}, "c7538c4adf04420d819816e11fccecfe": {"views": []}, "9d2dd1e4172c492293f921939bea1236": {"views": []}, "838063d4e314486d9c45a29115a555f4": {"views": []}, "f611935e9a7f46c08f359ad46a85fdb7": {"views": []}, "92f83b2c49b74dc49a62feed547d9091": {"views": []}, "5e0fb03ecb51477f8afa8101cbf9a74e": {"views": []}, "3503b7cea00145668b18db8bba5e2197": {"views": []}, "dcdc3a7f524d4e95b377d409d12174af": {"views": []}, "2bc66b4414fd4f918c7062875ce13d26": {"views": []}, "28c1dd8f20fe4dfa9dbcdf6d430e8daf": {"views": []}, "73db92a798c84edeb0e079968ca6b555": {"views": []}, "906614267fdf453bba3e673de0ff9777": {"views": []}, "2aae7ec099914196b5c47ddc5e71cc5e": {"views": []}, "a3f02721348b471fae8d083b7dac2c6e": {"views": [{"cell_index": 6}]}, "4f9d57025ebd4e5c9e82c8087d8e9a50": {"views": []}, "77aca3704d4b48cbb4795150ebac8eae": {"views": []}, "0ace5bc70f274d719b5db66872f864aa": {"views": []}, "51da009387fe43f892b21823cc308b8d": {"views": [{"cell_index": 7}]}, "f3633625776e497581605af2694cf89a": {"views": []}, "1970dbc873df4424895268492564fcdf": {"views": []}, "ec120090afca49249f6c41c0bad4381f": {"views": []}, "0ed8d5e4d3614ec299a701f43201de3c": {"views": []}, "d9abc07589b54241a0543d96de08e39e": {"views": []}, "a633faea5f4e4351bf10a9b5c2770fa9": {"views": []}, "38b20ffb18cb4414830c19bfca8ae7d9": {"views": []}, "a4929346a3c34250ae1afc8d792de31d": {"views": []}, "0ccc085109f949cdbab9ca636e729c2f": {"views": []}, "886fd7262b3b43a59539343235c83a11": {"views": []}, "b6ac3dfc2fcc432bb17abcac347d2e7b": {"views": []}, "046f53f06abd44f991d42b3717575c5c": {"views": []}, "cebe792d1ba9478797269a76f5646f77": {"views": [{"cell_index": 7}]}, "aa92bed88c7c4aa0a6c7468bcdf3564c": {"views": []}, "df310dad55be433c85eeb40cf53fe539": {"views": []}, "1aed39fe7cdd48dbb70d0263dc14aaca": {"views": []}, "a823a9453f5a47e0b9cbc07fc6c0644a": {"views": []}, "99dd2f5f844b4023872e95337aa930d3": {"views": []}}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}