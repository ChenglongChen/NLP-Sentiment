{"nbformat_minor": 0, "cells": [{"source": "## Upload Review Data using AzureML\n\nCreate a batch file and execute:\n    \n```\ncd \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\"\nAzCopy /Source:C:\\_ilia_share\\amazon_prod_reviews_clean\\raw /Dest:https://ikcentralusstore.blob.core.windows.net/amazonrev /DestKey:dLR5lH2QN/ejGmyD61nQoh7Cc2DW8jIKhR5n5uvGu8+H3Qem4J0XzWG1/7XtBxmVlWr+y/GNRlwX4Km5YU68sg== /Pattern:\"aggressive_dedup.json\"\npause\n```", "cell_type": "markdown", "metadata": {}}, {"source": "## Load Review Data (from Blob)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# Idea courtesy of Thomas D.\nimport time\nSTIME = { \"start\" : time.time() }\n\ndef tic():\n    STIME[\"start\"] = time.time()\n\ndef toc():\n    elapsed = time.time() - STIME[\"start\"]\n    print(\"%.2f seconds elasped\" % elapsed)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>34</td><td>application_1469453428769_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ikclus.ftd4jbtqjxzuhd0uvhsmx0be3e.gx.internal.cloudapp.net:8088/proxy/application_1469453428769_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.14:30060/node/containerlogs/container_e04_1469453428769_0012_01_000001/spark\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "Creating HiveContext as 'sqlContext'\nSparkContext and HiveContext created. Executing user code ...\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# paths\nblob = \"wasb://amazonrev@ikcentralusstore.blob.core.windows.net\"\njson_dta = blob + \"/aggressive_dedup.json\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 154, "cell_type": "code", "source": "# load data\njsonFile = sqlContext.read.json(json_dta)\njsonFile.registerTempTable(\"reviews\")\n\nprint(type(jsonFile)) #  <class 'pyspark.sql.dataframe.DataFrame'>\njsonFile.show(5)\n\n# Note: also load the IMDB data at some point\n# ...", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|      asin|helpful|overall|          reviewText| reviewTime|          reviewerID|   reviewerName|             summary|unixReviewTime|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|B003UYU16G| [0, 0]|    5.0|It is and does ex...|11 21, 2012|A00000262KYZUE4J5...| Steven N Elich|Does what it's su...|    1353456000|\n|B005FYPK9C| [0, 0]|    5.0|I was sketchy at ...| 01 8, 2013|A000008615DZQRRI9...|      mj waldon|           great buy|    1357603200|\n|B000VEBG9Y| [0, 0]|    3.0|Very mobile produ...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great product but...|    1395619200|\n|B001EJMS6K| [0, 0]|    4.0|Easy to use a mob...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great inexpensive...|    1395619200|\n|B003XJCNVO| [0, 0]|    4.0|Love this feeder....|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great feeder. Wou...|    1395619200|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "## Examine some of the reviews", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": 155, "cell_type": "code", "source": "%%sql \nSELECT overall, reviewText\nFROM reviews\nLIMIT 10", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 156, "cell_type": "code", "source": "%%sql \nSELECT overall, COUNT(overall) as freq\nFROM reviews\nGROUP BY overall\nORDER by -freq", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 157, "cell_type": "code", "source": "# Create a dataframe of our reviews\n# To analyse class imbalance\nreviews =  sqlContext.sql(\"SELECT \" + \n                          \"CASE WHEN overall < 3 THEN 'low' \" +\n                          \"WHEN overall > 3 THEN 'high' ELSE 'mid' END as label, \" + \n                          \"reviewText as sentences \" + \n                          \"FROM reviews\")\n\ntally = reviews.groupBy(\"label\").count()\ntally.show()\n\n#mid| 7,039,272\n#low|10,963,811\n#high|64,453,794", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------+\n|label|   count|\n+-----+--------+\n|  mid| 7039272|\n|  low|10963811|\n| high|64453794|\n+-----+--------+"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Let's look at some reviews to see how clean they are\n# there seems to be lots of html formatting\nfor c,r in enumerate(reviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Some very basic cleaning\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType, DoubleType \nfrom bs4 import BeautifulSoup\n\ndef cleanerHTML(line):\n    # html formatting\n    html_clean = BeautifulSoup(line, \"lxml\").get_text().lower()\n    # remove any double spaces, line-breaks, etc.\n    return \" \".join(html_clean.split())\n\ncleaner = UserDefinedFunction(cleanerHTML, StringType())\n\ndef labelForResults(s):\n    # string label to numeric\n    if s == 'low':\n        return 0.0\n    elif s == 'high':\n        return 1.0\n    else:\n        return -1.0\n        \nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\ncleanedReviews = reviews.select(reviews.label,\n                                label(reviews.label).alias('sentiment'), \n                                cleaner(reviews.sentences).alias('sentences'))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# A bit cleaner ...\nfor c,r in enumerate(cleanedReviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n#cleanedReviews.show()\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Equalise classes \nneg_rev = cleanedReviews.filter(\"sentiment = 0.0\")\npos_rev = cleanedReviews.filter(\"sentiment = 1.0\").limit(neg_rev.count())\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n# Save data\nallData = pos_rev.unionAll(neg_rev)\nprint(allData.count()) # 21,927,622 ( = 10,963,811 * 2)\n\nallDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData.write.json(allDataLoc)\n\"\"\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Load Clean Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "allDataLoc = blob + \"/cleaned_equal_classes.json\"\nallData = sqlContext.read.json(allDataLoc)\n\ndata_count = allData.count()\nprint(data_count)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Take 100,000\nsub_sample = 10000\nsub_sample_ratio = float(sub_sample)/float(data_count)\n\nprint(sub_sample_ratio)\nprint(type(allData))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# sub_sample -> sample(boolean withReplacement, double fraction, long seed)\nallData = allData.sample(False, sub_sample_ratio, 12345)\n\n# split intro training and test (50%, 50%)\ntrainingData, testData = allData.randomSplit([0.5, 0.5])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "trainingDataLoc = blob + \"/training_1mill.json\"\ntestDataLoc = blob + \"/testing_1mill.json\"", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "# Save\n#trainingData.write.mode(SaveMode.Overwrite).json(trainingDataLoc)\n#testData.write.mode(SaveMode.Overwrite).json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Load\ntrainingData = sqlContext.read.json(trainingDataLoc)\ntestData = sqlContext.read.json(testDataLoc)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "trainingData.cache()\ntestData.cache()\n\nprint(trainingData.count())\nprint(testData.count())", "outputs": [{"output_type": "stream", "name": "stdout", "text": "500349\n499826"}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "trainingData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|!i recommend this...|      1.0|\n| high|\" duty, honor, co...|      1.0|\n| high|\" ok let first st...|      1.0|\n| high|\"a deadly justice...|      1.0|\n| high|\"a dirty job\" is ...|      1.0|\n| high|\"a man of god\" is...|      1.0|\n| high|\"a practical book...|      1.0|\n| high|\"a widow's story\"...|      1.0|\n| high|\"abraham's burden...|      1.0|\n| high|\"always said if i...|      1.0|\n| high|\"american fool\" b...|      1.0|\n| high|\"antsy does time\"...|      1.0|\n| high|\"anyone who leads...|      1.0|\n| high|\"athlete/warrior\"...|      1.0|\n| high|\"better living th...|      1.0|\n| high|\"bob cornuke writ...|      1.0|\n| high|\"changing seasons...|      1.0|\n| high|\"city of angels: ...|      1.0|\n| high|\"cleopatra\" was t...|      1.0|\n| high|\"courage\" by dais...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "testData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+\n|label|           sentences|sentiment|\n+-----+--------------------+---------+\n| high|\" to sheldon and ...|      1.0|\n| high|\"...those men who...|      1.0|\n| high|\"a prayer for the...|      1.0|\n| high|\"a seacat's love\"...|      1.0|\n| high|\"amazing product ...|      1.0|\n| high|\"aves - the age o...|      1.0|\n| high|\"butera does it a...|      1.0|\n| high|\"c'era una volta ...|      1.0|\n| high|\"circle william\" ...|      1.0|\n| high|\"crush proof\" is ...|      1.0|\n| high|\"die unendliche g...|      1.0|\n| high|\"don't let fear h...|      1.0|\n| high|\"du hast\" means y...|      1.0|\n| high|\"feedback\" is an ...|      1.0|\n| high|\"fling\" is one of...|      1.0|\n| high|\"forgiving maximo...|      1.0|\n| high|\"gangster governm...|      1.0|\n| high|\"gone girl\" just ...|      1.0|\n| high|\"good parents bad...|      1.0|\n| high|\"great price with...|      1.0|\n+-----+--------------------+---------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "## 1. TFIDF", "cell_type": "markdown", "metadata": {}}, {"execution_count": 148, "cell_type": "code", "source": "\"\"\" Pipeline for feature selection and classification\nUsing:\n\nhttps://spark.apache.org/docs/1.5.2/ml-features.html\nhttps://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html\nhttp://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionModel\nhttp://nlp.stanford.edu/IR-book/html/htmledition/document-and-query-weighting-schemes-1.html#sec:querydocweighting\n\nAttempting to replicate: \n\nclass sklearn.feature_extraction.text.TfidfVectorizer(input='content', encoding='utf-8',\ndecode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, \ntokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \nngram_range=(1, 3), max_df=1.0, min_df=1, max_features=40000, vocabulary=None, \nbinary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, \nsmooth_idf=True, sublinear_tf=True)\n\nI think only sublinear_tf and ngram_range need to be modified\n\n(https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py)\nif self.sublinear_tf:\n    np.log(X.data, X.data)\n    X.data += 1\n            \n\"\"\"\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.sql.functions import col, udf\nfrom itertools import chain\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.mllib.linalg import Vectors, VectorUDT\nimport numpy as np\n\nnumfeat = 40000\n\n########################\n# 1. Feature-extraction\n########################\n\ndef concat(type):\n    \"\"\" UDF to concatenate lists across columns to create\n    an n-gram range. To reproduce ngram_range=(1,3) from sklearn\n    \"\"\"\n    def concat_(*args):\n        return list(chain(*args))\n    return udf(concat_, ArrayType(type))                   \nconcat_string_arrays = concat(StringType())\n\n\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"sentiment_idx\")\ntokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\nbiGram = NGram(inputCol = \"words\", n=2, outputCol = \"2gram\")\ntriGram = NGram(inputCol = \"words\", n=3, outputCol = \"3gram\")\nhashingtf  = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=numfeat)\n# UDF to apply sub-linear scaling on sparse vectors tf\nvector_udf = udf(lambda sv: Vectors.sparse(sv.size, dict(zip(sv.indices, np.log(sv.values) + 1))), VectorUDT())\nidf = IDF(inputCol=\"logRawFeatures\", outputCol=\"features\")\n\n#######\n# Train\n#######\nindexerModel = indexer.fit(trainingData)\ntrainingDataIx = indexerModel.transform(trainingData)\ntokenized_train = tokenizer.transform(trainingDataIx)\n\nbiGram_train = biGram.transform(tokenized_train)\ntriGram_train = triGram.transform(biGram_train)\nngrammed_train = triGram_train.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_train = hashingtf.transform(ngrammed_train)\nsublintf_train = hashed_train.withColumn('logRawFeatures', vector_udf(hashed_train.rawFeatures))\n\nidfModel = idf.fit(sublintf_train)\nidf_train = idfModel.transform(sublintf_train)\nidf_train.first()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+---------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|label|           sentences|sentiment|sentiment_idx|               words|               2gram|               3gram|              ngrams|         rawFeatures|      logRawFeatures|            features|\n+-----+--------------------+---------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n| high|!i recommend this...|      1.0|          1.0|[!i, recommend, t...|[!i recommend, re...|[!i recommend thi...|[!i, recommend, t...|(40000,[277,1056,...|(40000,[277,1056,...|(40000,[277,1056,...|\n| high|\" duty, honor, co...|      1.0|          1.0|[\", duty,, honor,...|[\" duty,, duty, h...|[\" duty, honor,, ...|[\", duty,, honor,...|(40000,[34,97,432...|(40000,[34,97,432...|(40000,[34,97,432...|\n| high|\" ok let first st...|      1.0|          1.0|[\", ok, let, firs...|[\" ok, ok let, le...|[\" ok let, ok let...|[\", ok, let, firs...|(40000,[34,38,45,...|(40000,[34,38,45,...|(40000,[34,38,45,...|\n| high|\"a deadly justice...|      1.0|          1.0|[\"a, deadly, just...|[\"a deadly, deadl...|[\"a deadly justic...|[\"a, deadly, just...|(40000,[105,220,3...|(40000,[105,220,3...|(40000,[105,220,3...|\n| high|\"a dirty job\" is ...|      1.0|          1.0|[\"a, dirty, job\",...|[\"a dirty, dirty ...|[\"a dirty job\", d...|[\"a, dirty, job\",...|(40000,[0,99,105,...|(40000,[0,99,105,...|(40000,[0,99,105,...|\n| high|\"a man of god\" is...|      1.0|          1.0|[\"a, man, of, god...|[\"a man, man of, ...|[\"a man of, man o...|[\"a, man, of, god...|(40000,[53,55,97,...|(40000,[53,55,97,...|(40000,[53,55,97,...|\n| high|\"a practical book...|      1.0|          1.0|[\"a, practical, b...|[\"a practical, pr...|[\"a practical boo...|[\"a, practical, b...|(40000,[646,972,1...|(40000,[646,972,1...|(40000,[646,972,1...|\n| high|\"a widow's story\"...|      1.0|          1.0|[\"a, widow's, sto...|[\"a widow's, wido...|[\"a widow's story...|[\"a, widow's, sto...|(40000,[25,38,45,...|(40000,[25,38,45,...|(40000,[25,38,45,...|\n| high|\"abraham's burden...|      1.0|          1.0|[\"abraham's, burd...|[\"abraham's burde...|[\"abraham's burde...|[\"abraham's, burd...|(40000,[15,58,97,...|(40000,[15,58,97,...|(40000,[15,58,97,...|\n| high|\"always said if i...|      1.0|          1.0|[\"always, said, i...|[\"always said, sa...|[\"always said if,...|[\"always, said, i...|(40000,[2,45,65,9...|(40000,[2,45,65,9...|(40000,[2,45,65,9...|\n| high|\"american fool\" b...|      1.0|          1.0|[\"american, fool\"...|[\"american fool\",...|[\"american fool\" ...|[\"american, fool\"...|(40000,[5,38,69,9...|(40000,[5,38,69,9...|(40000,[5,38,69,9...|\n| high|\"antsy does time\"...|      1.0|          1.0|[\"antsy, does, ti...|[\"antsy does, doe...|[\"antsy does time...|[\"antsy, does, ti...|(40000,[97,105,17...|(40000,[97,105,17...|(40000,[97,105,17...|\n| high|\"anyone who leads...|      1.0|          1.0|[\"anyone, who, le...|[\"anyone who, who...|[\"anyone who lead...|[\"anyone, who, le...|(40000,[97,358,41...|(40000,[97,358,41...|(40000,[97,358,41...|\n| high|\"athlete/warrior\"...|      1.0|          1.0|[\"athlete/warrior...|[\"athlete/warrior...|[\"athlete/warrior...|[\"athlete/warrior...|(40000,[97,1893,1...|(40000,[97,1893,1...|(40000,[97,1893,1...|\n| high|\"better living th...|      1.0|          1.0|[\"better, living,...|[\"better living, ...|[\"better living t...|[\"better, living,...|(40000,[97,158,21...|(40000,[97,158,21...|(40000,[97,158,21...|\n| high|\"bob cornuke writ...|      1.0|          1.0|[\"bob, cornuke, w...|[\"bob cornuke, co...|[\"bob cornuke wri...|[\"bob, cornuke, w...|(40000,[55,344,40...|(40000,[55,344,40...|(40000,[55,344,40...|\n| high|\"changing seasons...|      1.0|          1.0|[\"changing, seaso...|[\"changing season...|[\"changing season...|[\"changing, seaso...|(40000,[97,105,12...|(40000,[97,105,12...|(40000,[97,105,12...|\n| high|\"city of angels: ...|      1.0|          1.0|[\"city, of, angel...|[\"city of, of ang...|[\"city of angels:...|[\"city, of, angel...|(40000,[20,97,572...|(40000,[20,97,572...|(40000,[20,97,572...|\n| high|\"cleopatra\" was t...|      1.0|          1.0|[\"cleopatra\", was...|[\"cleopatra\" was,...|[\"cleopatra\" was ...|[\"cleopatra\", was...|(40000,[97,105,10...|(40000,[97,105,10...|(40000,[97,105,10...|\n| high|\"courage\" by dais...|      1.0|          1.0|[\"courage\", by, d...|[\"courage\" by, by...|[\"courage\" by dai...|[\"courage\", by, d...|(40000,[97,450,80...|(40000,[97,450,80...|(40000,[97,450,80...|\n+-----+--------------------+---------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "### tfidf variants:\n\n![alt text](http://nlp.stanford.edu/IR-book/html/htmledition/img462.png \"TFs\")", "cell_type": "markdown", "metadata": {}}, {"execution_count": 146, "cell_type": "code", "source": "\"\"\"\n###########################\n# Example: Apply sub-linear\n###########################\nfrom pyspark.mllib.linalg import Vectors, VectorUDT\nimport numpy as np\n\ntesty = hashed_train.first()['rawFeatures']\n\nprint(type(testy))\nprint(type(testy.values))\nprint(testy)\n\nvector_udf = udf(lambda sv: Vectors.sparse(sv.size, dict(zip(sv.indices, np.log(sv.values) + 10))), VectorUDT())\nsublintf_train = hashed_train.withColumn('sublintf', vector_udf(hashed_train.rawFeatures))\n\ntesty2 = sublintf_train.first()['sublintf']\nprint(testy2)\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<class 'pyspark.mllib.linalg.SparseVector'>\n<type 'numpy.ndarray'>\n(40000,[277,1056,1128,2165,3370,3371,3500,3707,4051,4486,4668,4846,5627,5646,6272,6944,7183,8088,9207,9541,9581,10564,11966,12365,12620,12709,13250,14081,15914,16727,16890,16897,16956,20127,20883,21925,22837,23051,23444,23895,23967,24280,24738,25310,25969,26033,26302,27086,29182,29509,29593,31576,32107,32586,34076,34380,34466,34619,35240,35615,36103,36715,37023,37075,37368,38027,38092,38621,39070,39352],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n(40000,[277,1056,1128,2165,3370,3371,3500,3707,4051,4486,4668,4846,5627,5646,6272,6944,7183,8088,9207,9541,9581,10564,11966,12365,12620,12709,13250,14081,15914,16727,16890,16897,16956,20127,20883,21925,22837,23051,23444,23895,23967,24280,24738,25310,25969,26033,26302,27086,29182,29509,29593,31576,32107,32586,34076,34380,34466,34619,35240,35615,36103,36715,37023,37075,37368,38027,38092,38621,39070,39352],[10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.6931471806,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.6931471806,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0,10.0])"}], "metadata": {"collapsed": false}}, {"execution_count": 151, "cell_type": "code", "source": "######\n# Test\n######\ntestDataIx = indexerModel.transform(testData)\ntokenized_test = tokenizer.transform(testDataIx)\n\nbiGram_test = biGram.transform(tokenized_test)\ntriGram_test = triGram.transform(biGram_test)\nngrammed_test = triGram_test.withColumn(\"ngrams\", concat_string_arrays(\n        col(\"words\"),\n        col(\"2gram\"),\n        col(\"3gram\")))\nhashed_test = hashingtf.transform(ngrammed_test)\nsublintf_test = hashed_test.withColumn('logRawFeatures', vector_udf(hashed_test.rawFeatures))\nidf_test = idfModel.transform(sublintf_test)\n\n#idf_test.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 153, "cell_type": "code", "source": "# 2A. Classifier (Logistic Regression)\nclassi = LogisticRegression(labelCol=\"sentiment_idx\", featuresCol=\"features\")\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # 76.77 success rate\n\n\"\"\"\nStandard: 76.77 success rate\nWith ngrams(1,3): 88.17 success rate\nWith ngrams + sublineartf: 88.32 success rate\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "88.32 success rate\n'\\nStandard: 76.77 success rate\\nWith ngrams(1,3): 88.17 success rate\\nWith ngrams + sublineartf: ?\\n'\n/usr/hdp/current/spark-client/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n  warnings.warn(\"weights is deprecated. Use coefficients instead.\")"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# 2C. Classifier (GBTClassifier)\nclassi = GBTClassifier(labelCol=\"sentiment_idx\", featuresCol=\"features\", numClasses=2)\ntfidfModel = classi.fit(idf_train)\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = sentiment_idx)\"\"\").count()\nnumInspections = numSuccesses + pred.where(\"\"\"(prediction != sentiment_idx)\"\"\").count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # ? success rate", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# 3. Evaluation\npred.select(col('prediction'),col('sentiment_idx')).show()", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {"c103bee778ba456b89a6a7a2da76335c": {"views": []}, "804f260ad5684d1091f1d31d977b563f": {"views": []}, "e5d6c61792a648a9b7c2b2533417f06c": {"views": []}, "80af13b967dc459183c22d2ecfc1dfb4": {"views": []}, "db69222e5f384e83a0eb246e5285d3c7": {"views": []}, "e491652c83d64d228c5d6c73cf4a0e8a": {"views": []}, "039997d408e84f40b78f37c993eb9b8b": {"views": []}, "a2e29eea1ae1474bb245d1663ae4395e": {"views": []}, "67e069fd72cd4b9ab6d23f5526341377": {"views": [{"cell_index": 6}]}, "710bf8465c5f4c89af5685e417edbaa4": {"views": []}, "240bf904d1164081afbe636bda920205": {"views": []}, "487d92d19d3a46dab95b74bc54e0ebfd": {"views": []}, "be45eb7f7ea5467eae127dffd52a9758": {"views": []}, "51ff80050f5947d3b369d53064d10ebc": {"views": []}, "95fe29df6aab451d95511de34870869b": {"views": []}, "df2f9ba3efc44534b566707b80c6dd06": {"views": []}, "198e70e397bf4ebcac5aebea4b07dbe6": {"views": []}, "80695e48077f4f99842173695e196a03": {"views": []}, "4477c2c6d481413a988481f65bc88523": {"views": []}, "56c6548c5ce94dc595450abc68ca2156": {"views": []}, "51116f717c4d44a6848d5ff9ef5b4508": {"views": []}, "6f960bf81ddd4d6fb94850163c5b10b6": {"views": []}, "6b1f48010c7e4a16852ff4a8f2e1d289": {"views": []}, "265378c026b041d4b144c67673c858ae": {"views": []}, "2e90a23d45ac499e8f86283d9c2e7d2f": {"views": []}, "9282abae5669428481c9a6bcddd8f0df": {"views": []}, "d8df5548fac846828c11360f152606df": {"views": []}, "e7301e8f6447463486e2de904f072e64": {"views": []}, "b42a94f3d7444200bf6293866731551c": {"views": []}, "c5fa2ef8a12c46c4a43842441350c3ea": {"views": []}, "65b96c7f78f040a3b40f486a776d36d5": {"views": []}, "8b0f138ea35c45e18c40b091ade2bac9": {"views": []}, "424aa32d4e6c4382b2b103be892f3a00": {"views": []}, "47afaa6dc13c46c0b54ee585b50cc052": {"views": []}, "f85607fc2ef7494696efe0f4106858e9": {"views": []}, "a2c68a0bb2294501843b288bf3cb91ed": {"views": []}, "fa9bb88ee4fc4984a82571127c7de33f": {"views": []}, "39eacea6074a4fbdb7a17bb9931ba259": {"views": []}, "76c947565202488e97c3a415f179f043": {"views": []}, "af6a1131c24a463e9e67689fe3fa38bb": {"views": []}, "7ced9c27297b4f7db92dd7f56bde3547": {"views": [{"cell_index": 7}]}, "21ceabfc94964d08add5a13f6d8c7be2": {"views": []}, "b523f557d404485db8707fecd5bef321": {"views": []}, "036827ba5014459eb95b2defaeb4f65f": {"views": []}, "7df0a6629260436b9e8ec4c59cc4df20": {"views": []}, "6c25098389644317950224f4941693e1": {"views": []}, "d8f6a110c20843afaf3aebfeac924b8f": {"views": []}, "5c297e1afd2147f98dc5073422485576": {"views": []}, "f0320b322bcb4df386fbf1d380b572b2": {"views": []}, "354b4ccdf33643e9bd483e1ccffb5c18": {"views": []}, "ed9b2a27d6204a81b6ea50e6e46a58f6": {"views": []}, "06307f782ab047448b8e4f7090610d92": {"views": []}, "1269b1a24abf40fd894092e775f9a6eb": {"views": []}, "a43235c8bf84432b85e44c887c469244": {"views": []}, "8526216b2f2b4b3490316fca6547084a": {"views": []}, "96f77f997f1d4a318399a804db16c446": {"views": []}, "6323d9161106435f8285cdee7eeb8bb5": {"views": []}, "0fcf24862a194606aef82a5c61d3f751": {"views": [{"cell_index": 7}]}, "b5f65b6fc27347d58d242270acd4e86c": {"views": []}, "311fb696445042d4b817d7397477f2dc": {"views": []}, "d2cf5ddfb1ac4f6abd07c7505ac49724": {"views": []}, "57a8cf17518e483a8b082c2d4a378afb": {"views": []}, "bfd0001fe5e8460eaddfbc9eddccc0f4": {"views": []}, "5cedf57dacf74284b52f4b8762bff43f": {"views": []}, "f101766a6cbb47b8ab6230878116e628": {"views": []}, "6a3e491b0bfe4dbeba613fb8822d7187": {"views": []}, "0fc67840be1e408f8bc8d65330dc68c9": {"views": []}, "711811b6489c4151a6d98f79c2c70f7a": {"views": []}, "ad9ac46ee09243f88d96c446caf902a8": {"views": []}, "829d6cae2c7c49eda408e9cd8f8c05f2": {"views": []}, "1cb33b5babd546168a1de7d4382c0c5a": {"views": []}, "c172225eb534465abc168ca1aaadbbb1": {"views": []}, "27b6065c386d416581159cac0a6afb64": {"views": []}, "0375a1364793410582cfcc661403338b": {"views": []}, "09f126f977b64638bef803c12d4fc756": {"views": []}, "c87ae0404ecc43beafdcdc724c9d779f": {"views": []}, "13af7c8e8f1b4fc386437dc61f167de5": {"views": []}, "e51dc5668c174e4cb55dc55a3e891937": {"views": []}, "bcff5e77a23e411cb4f75a5afd5848f4": {"views": []}, "fa388f881fcd46089e183392c7d39d6b": {"views": []}, "566a0b81297e400baac54402696531e0": {"views": [{"cell_index": 6}]}, "4303d979f96749c69d6d24155b2d17c7": {"views": []}, "feaa36e83edf4532b73565bca58be3ba": {"views": []}, "1fe539e478cc4e4bbd62f7effa027fc5": {"views": []}, "92d43083944549bf847011a75f23472a": {"views": []}, "2cbfab1652ec42ccbd297f1f5ae2e72c": {"views": []}, "08adc1325d4942c4b1d985c6cc7bc12a": {"views": []}, "7ded89c53bc44f6695948af45ba9c0ce": {"views": []}}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}