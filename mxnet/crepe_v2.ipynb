{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]\n",
    "ctx = mx.gpu(0)\n",
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128 \n",
    "NUM_FILTERS = 256\n",
    "EPOCHS = 10000\n",
    "SD = 0.05  # std for gaussian distribution\n",
    "NOUTPUT = 2  # good or bad\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='crepe_inram_onegpu.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        # print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('\\nsaved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_frame(infile, shuffle = False):\n",
    "    print(\"processing data frame: %s\" % infile)\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "    \n",
    "    # 3.6 mill is too much, use 2 mill (keep same ratio)\n",
    "    if \"test\" in infile:\n",
    "        maxrows = int(2097152/9)  # 16,384 batches\n",
    "    elif \"train\" in infile:\n",
    "        maxrows = int(2097152)\n",
    "\n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'],\n",
    "                     nrows=maxrows)\n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='bool')\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET), dtype='bool'), columns=ALPHABET)\n",
    "    print(\"finished processing data frame: %s\" % infile)\n",
    "    print(\"data contains %d obs\" % df.shape[0])\n",
    "    batch_size = df.shape[0]\n",
    "    # Create encoding\n",
    "    X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='bool')\n",
    "    # Main loop\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if (ti+1) % (100*1000) == 0:\n",
    "            print(\"Processed: \", ti+1)\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch], dtype='bool')\n",
    "                \n",
    "    # Return as a DataBatch\n",
    "    #return DataBatch(data=[mx.nd.array(X_split)],\n",
    "    #                 label=[mx.nd.array(y_split[ti + 1 - batch_size:ti + 1])])\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_crepe():\n",
    "    \"\"\"\n",
    "    Replicating: https://github.com/zhangxiangxiao/Crepe/blob/master/train/config.lua\n",
    "    \"\"\"\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "    # 1. alphabet x 1014\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=input_x, kernel=(7, 69), num_filter=NUM_FILTERS)\n",
    "    relu1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"relu\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=relu1, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 2. 336 x 256\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(7, 1), num_filter=NUM_FILTERS)\n",
    "    relu2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"relu\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=relu2, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 3. 110 x 256\n",
    "    conv3 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu3 = mx.symbol.Activation(\n",
    "        data=conv3, act_type=\"relu\")\n",
    "    # 4. 108 x 256\n",
    "    conv4 = mx.symbol.Convolution(\n",
    "        data=relu3, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu4 = mx.symbol.Activation(\n",
    "        data=conv4, act_type=\"relu\")\n",
    "    # 5. 106 x 256\n",
    "    conv5 = mx.symbol.Convolution(\n",
    "        data=relu4, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu5 = mx.symbol.Activation(\n",
    "        data=conv5, act_type=\"relu\")\n",
    "    # 6. 104 x 256\n",
    "    conv6 = mx.symbol.Convolution(\n",
    "        data=relu5, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu6 = mx.symbol.Activation(\n",
    "        data=conv6, act_type=\"relu\")\n",
    "    pool6 = mx.symbol.Pooling(\n",
    "        data=relu6, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 34 x 256\n",
    "    flatten = mx.symbol.Flatten(data=pool6)\n",
    "    # 7.  8704\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=1024)\n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"relu\")\n",
    "    drop1 = mx.sym.Dropout(act_fc1, p=0.5)\n",
    "    # 8. 1024\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=drop1, num_hidden=1024)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type=\"relu\")\n",
    "    drop2 = mx.sym.Dropout(act_fc2, p=0.5)\n",
    "    # 9. 1024\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=drop2, num_hidden=NOUTPUT)\n",
    "    crepe = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "    return crepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: amazon_review_polarity_train.csv\n",
      "finished processing data frame: amazon_review_polarity_train.csv\n",
      "data contains 2097152 obs\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data_frame('amazon_review_polarity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y\n",
    "# 147 GB, 69% with 2,097,000 mill obs (3.6 will be too much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = create_crepe(), \n",
    "    num_epoch = EPOCHS,  # number of training rounds\n",
    "    learning_rate = 0.01,  # learning rate\n",
    "    momentum = 0.9,   # momentum for sgd\n",
    "    wd = 0.00001,  # weight decay for reg\n",
    "    initializer = mx.init.Normal(sigma=SD)  # init with sd of 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_checkp_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2016-08-23 05:34:21,667 - root - INFO - Auto-select kvstore type = local_allreduce_cpu\n",
    "2016-08-23 05:34:21,822 - root - INFO - Start training with [gpu(0), gpu(1), gpu(2), gpu(3)]\n",
    "2016-08-23 05:38:04,459 - root - INFO - Epoch[0] Batch [50]\tSpeed: 3951.01 samples/sec\tTrain-accuracy=0.501406\n",
    "2016-08-23 05:40:48,198 - root - INFO - Epoch[0] Batch [100]\tSpeed: 3908.64 samples/sec\tTrain-accuracy=0.492031\n",
    "2016-08-23 05:43:31,387 - root - INFO - Epoch[0] Batch [150]\tSpeed: 3922.22 samples/sec\tTrain-accuracy=0.497812\n",
    "2016-08-23 05:46:17,117 - root - INFO - Epoch[0] Batch [200]\tSpeed: 3861.70 samples/sec\tTrain-accuracy=0.499219\n",
    "2016-08-23 05:49:03,273 - root - INFO - Epoch[0] Batch [250]\tSpeed: 3852.15 samples/sec\tTrain-accuracy=0.505313\n",
    "2016-08-23 05:51:49,802 - root - INFO - Epoch[0] Batch [300]\tSpeed: 3843.20 samples/sec\tTrain-accuracy=0.501719\n",
    "2016-08-23 05:54:35,424 - root - INFO - Epoch[0] Batch [350]\tSpeed: 3864.22 samples/sec\tTrain-accuracy=0.499375\n",
    "2016-08-23 05:57:22,983 - root - INFO - Epoch[0] Batch [400]\tSpeed: 3819.53 samples/sec\tTrain-accuracy=0.506250\n",
    "2016-08-23 06:00:07,595 - root - INFO - Epoch[0] Batch [450]\tSpeed: 3887.95 samples/sec\tTrain-accuracy=0.502031\n",
    "2016-08-23 06:02:52,483 - root - INFO - Epoch[0] Batch [500]\tSpeed: 3881.40 samples/sec\tTrain-accuracy=0.511094\n",
    "2016-08-23 06:05:36,200 - root - INFO - Epoch[0] Batch [550]\tSpeed: 3909.21 samples/sec\tTrain-accuracy=0.506406\n",
    "2016-08-23 06:08:22,447 - root - INFO - Epoch[0] Batch [600]\tSpeed: 3850.76 samples/sec\tTrain-accuracy=0.517031\n",
    "2016-08-23 06:11:07,180 - root - INFO - Epoch[0] Batch [650]\tSpeed: 3885.48 samples/sec\tTrain-accuracy=0.498906\n",
    "2016-08-23 06:13:54,270 - root - INFO - Epoch[0] Batch [700]\tSpeed: 3830.25 samples/sec\tTrain-accuracy=0.510000\n",
    "2016-08-23 06:16:38,315 - root - INFO - Epoch[0] Batch [750]\tSpeed: 3901.75 samples/sec\tTrain-accuracy=0.503750\n",
    "2016-08-23 06:19:22,877 - root - INFO - Epoch[0] Batch [800]\tSpeed: 3889.14 samples/sec\tTrain-accuracy=0.505000\n",
    "2016-08-23 06:22:10,806 - root - INFO - Epoch[0] Batch [850]\tSpeed: 3811.11 samples/sec\tTrain-accuracy=0.488906\n",
    "2016-08-23 06:24:57,601 - root - INFO - Epoch[0] Batch [900]\tSpeed: 3837.05 samples/sec\tTrain-accuracy=0.502188\n",
    "2016-08-23 06:27:41,450 - root - INFO - Epoch[0] Batch [950]\tSpeed: 3906.04 samples/sec\tTrain-accuracy=0.504062\n",
    "2016-08-23 06:30:27,716 - root - INFO - Epoch[0] Batch [1000]\tSpeed: 3849.28 samples/sec\tTrain-accuracy=0.509062\n",
    "2016-08-23 06:33:12,042 - root - INFO - Epoch[0] Batch [1050]\tSpeed: 3895.03 samples/sec\tTrain-accuracy=0.497500\n",
    "2016-08-23 06:35:58,604 - root - INFO - Epoch[0] Batch [1100]\tSpeed: 3842.78 samples/sec\tTrain-accuracy=0.492500\n",
    "2016-08-23 06:38:42,322 - root - INFO - Epoch[0] Batch [1150]\tSpeed: 3909.16 samples/sec\tTrain-accuracy=0.510469\n",
    "2016-08-23 06:41:30,394 - root - INFO - Epoch[0] Batch [1200]\tSpeed: 3807.89 samples/sec\tTrain-accuracy=0.492031\n",
    "2016-08-23 06:44:14,674 - root - INFO - Epoch[0] Batch [1250]\tSpeed: 3896.14 samples/sec\tTrain-accuracy=0.494375\n",
    "2016-08-23 06:47:02,858 - root - INFO - Epoch[0] Batch [1300]\tSpeed: 3805.36 samples/sec\tTrain-accuracy=0.502344\n",
    "2016-08-23 06:49:45,404 - root - INFO - Epoch[0] Batch [1350]\tSpeed: 3937.35 samples/sec\tTrain-accuracy=0.512188\n",
    "2016-08-23 06:52:33,546 - root - INFO - Epoch[0] Batch [1400]\tSpeed: 3806.33 samples/sec\tTrain-accuracy=0.508594\n",
    "2016-08-23 06:55:17,378 - root - INFO - Epoch[0] Batch [1450]\tSpeed: 3906.42 samples/sec\tTrain-accuracy=0.497812\n",
    "2016-08-23 06:58:00,380 - root - INFO - Epoch[0] Batch [1500]\tSpeed: 3926.36 samples/sec\tTrain-accuracy=0.495625\n",
    "2016-08-23 07:00:49,207 - root - INFO - Epoch[0] Batch [1550]\tSpeed: 3790.84 samples/sec\tTrain-accuracy=0.495625\n",
    "2016-08-23 07:03:31,723 - root - INFO - Epoch[0] Batch [1600]\tSpeed: 3938.07 samples/sec\tTrain-accuracy=0.492344\n",
    "2016-08-23 07:06:17,380 - root - INFO - Epoch[0] Batch [1650]\tSpeed: 3863.80 samples/sec\tTrain-accuracy=0.504219\n",
    "2016-08-23 07:09:06,976 - root - INFO - Epoch[0] Batch [1700]\tSpeed: 3773.67 samples/sec\tTrain-accuracy=0.489375\n",
    "2016-08-23 07:11:52,362 - root - INFO - Epoch[0] Batch [1750]\tSpeed: 3870.06 samples/sec\tTrain-accuracy=0.504531\n",
    "2016-08-23 07:14:37,066 - root - INFO - Epoch[0] Batch [1800]\tSpeed: 3886.14 samples/sec\tTrain-accuracy=0.495312\n",
    "2016-08-23 07:17:25,444 - root - INFO - Epoch[0] Batch [1850]\tSpeed: 3800.99 samples/sec\tTrain-accuracy=0.497656\n",
    "2016-08-23 07:20:12,398 - root - INFO - Epoch[0] Batch [1900]\tSpeed: 3833.39 samples/sec\tTrain-accuracy=0.497656\n",
    "2016-08-23 07:22:58,779 - root - INFO - Epoch[0] Batch [1950]\tSpeed: 3846.59 samples/sec\tTrain-accuracy=0.500156\n",
    "2016-08-23 07:25:45,132 - root - INFO - Epoch[0] Batch [2000]\tSpeed: 3847.22 samples/sec\tTrain-accuracy=0.511250\n",
    "2016-08-23 07:28:28,523 - root - INFO - Epoch[0] Batch [2050]\tSpeed: 3917.34 samples/sec\tTrain-accuracy=0.492031\n",
    "2016-08-23 07:31:14,592 - root - INFO - Epoch[0] Batch [2100]\tSpeed: 3854.21 samples/sec\tTrain-accuracy=0.507656\n",
    "2016-08-23 07:33:56,562 - root - INFO - Epoch[0] Batch [2150]\tSpeed: 3951.35 samples/sec\tTrain-accuracy=0.487031\n",
    "2016-08-23 07:36:42,016 - root - INFO - Epoch[0] Batch [2200]\tSpeed: 3868.12 samples/sec\tTrain-accuracy=0.507188\n",
    "2016-08-23 07:39:24,408 - root - INFO - Epoch[0] Batch [2250]\tSpeed: 3941.08 samples/sec\tTrain-accuracy=0.502500\n",
    "2016-08-23 07:42:07,263 - root - INFO - Epoch[0] Batch [2300]\tSpeed: 3929.88 samples/sec\tTrain-accuracy=0.496094\n",
    "2016-08-23 07:44:55,990 - root - INFO - Epoch[0] Batch [2350]\tSpeed: 3793.49 samples/sec\tTrain-accuracy=0.504062\n",
    "2016-08-23 07:47:42,301 - root - INFO - Epoch[0] Batch [2400]\tSpeed: 3848.21 samples/sec\tTrain-accuracy=0.496719\n",
    "2016-08-23 07:50:29,348 - root - INFO - Epoch[0] Batch [2450]\tSpeed: 3831.23 samples/sec\tTrain-accuracy=0.491563\n",
    "2016-08-23 07:53:11,974 - root - INFO - Epoch[0] Batch [2500]\tSpeed: 3935.41 samples/sec\tTrain-accuracy=0.498125\n",
    "2016-08-23 07:55:58,289 - root - INFO - Epoch[0] Batch [2550]\tSpeed: 3848.12 samples/sec\tTrain-accuracy=0.500313\n",
    "2016-08-23 07:58:43,556 - root - INFO - Epoch[0] Batch [2600]\tSpeed: 3872.87 samples/sec\tTrain-accuracy=0.520625\n",
    "2016-08-23 08:01:29,214 - root - INFO - Epoch[0] Batch [2650]\tSpeed: 3863.38 samples/sec\tTrain-accuracy=0.497969\n",
    "2016-08-23 08:04:14,497 - root - INFO - Epoch[0] Batch [2700]\tSpeed: 3873.60 samples/sec\tTrain-accuracy=0.502656\n",
    "2016-08-23 08:07:01,671 - root - INFO - Epoch[0] Batch [2750]\tSpeed: 3828.71 samples/sec\tTrain-accuracy=0.500938\n",
    "2016-08-23 08:09:46,490 - root - INFO - Epoch[0] Batch [2800]\tSpeed: 3883.07 samples/sec\tTrain-accuracy=0.489063\n",
    "2016-08-23 08:12:33,032 - root - INFO - Epoch[0] Batch [2850]\tSpeed: 3842.87 samples/sec\tTrain-accuracy=0.500313\n",
    "2016-08-23 08:15:17,720 - root - INFO - Epoch[0] Batch [2900]\tSpeed: 3886.47 samples/sec\tTrain-accuracy=0.504531\n",
    "2016-08-23 08:18:03,858 - root - INFO - Epoch[0] Batch [2950]\tSpeed: 3852.24 samples/sec\tTrain-accuracy=0.498281\n",
    "2016-08-23 08:20:49,171 - root - INFO - Epoch[0] Batch [3000]\tSpeed: 3871.42 samples/sec\tTrain-accuracy=0.507656\n",
    "2016-08-23 08:23:37,118 - root - INFO - Epoch[0] Batch [3050]\tSpeed: 3810.75 samples/sec\tTrain-accuracy=0.503437\n",
    "2016-08-23 08:26:23,233 - root - INFO - Epoch[0] Batch [3100]\tSpeed: 3852.73 samples/sec\tTrain-accuracy=0.495625\n",
    "2016-08-23 08:29:08,282 - root - INFO - Epoch[0] Batch [3150]\tSpeed: 3877.64 samples/sec\tTrain-accuracy=0.505313\n",
    "2016-08-23 08:31:52,980 - root - INFO - Epoch[0] Batch [3200]\tSpeed: 3885.92 samples/sec\tTrain-accuracy=0.513750\n",
    "2016-08-23 08:34:39,924 - root - INFO - Epoch[0] Batch [3250]\tSpeed: 3833.94 samples/sec\tTrain-accuracy=0.506719\n",
    "2016-08-23 08:37:28,635 - root - INFO - Epoch[0] Batch [3300]\tSpeed: 3793.45 samples/sec\tTrain-accuracy=0.498906\n",
    "2016-08-23 08:40:12,437 - root - INFO - Epoch[0] Batch [3350]\tSpeed: 3907.54 samples/sec\tTrain-accuracy=0.496094\n",
    "2016-08-23 08:42:58,313 - root - INFO - Epoch[0] Batch [3400]\tSpeed: 3858.68 samples/sec\tTrain-accuracy=0.496719\n",
    "...\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
