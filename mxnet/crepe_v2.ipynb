{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0)\n",
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128 \n",
    "NUM_FILTERS = 256\n",
    "EPOCHS = 10000\n",
    "SD = 0.05  # std for gaussian distribution\n",
    "NOUTPUT = 2  # good or bad\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='crepe_inram_onegpu.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        # print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('\\nsaved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_frame(infile, shuffle = False):\n",
    "    print(\"processing data frame: %s\" % infile)\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "    \n",
    "    # 3.6 mill is too much, use 2 mill (keep same ratio)\n",
    "    if \"test\" in infile:\n",
    "        maxrows = int(2097152/9)  # 16,384 batches\n",
    "    elif \"train\" in infile:\n",
    "        maxrows = int(2097152)\n",
    "\n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'],\n",
    "                     nrows=maxrows)\n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='bool')\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET), dtype='bool'), columns=ALPHABET)\n",
    "    print(\"finished processing data frame: %s\" % infile)\n",
    "    print(\"data contains %d obs\" % df.shape[0])\n",
    "    batch_size = df.shape[0]\n",
    "    # Create encoding\n",
    "    X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='bool')\n",
    "    # Main loop\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if (ti+1) % (100*1000) == 0:\n",
    "            print(\"Processed: \", ti+1)\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch], dtype='bool')\n",
    "                \n",
    "    # Return as a DataBatch\n",
    "    #return DataBatch(data=[mx.nd.array(X_split)],\n",
    "    #                 label=[mx.nd.array(y_split[ti + 1 - batch_size:ti + 1])])\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_crepe():\n",
    "    \"\"\"\n",
    "    Replicating: https://github.com/zhangxiangxiao/Crepe/blob/master/train/config.lua\n",
    "    \"\"\"\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "    # 1. alphabet x 1014\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=input_x, kernel=(7, 69), num_filter=NUM_FILTERS)\n",
    "    relu1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"relu\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=relu1, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 2. 336 x 256\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(7, 1), num_filter=NUM_FILTERS)\n",
    "    relu2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"relu\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=relu2, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 3. 110 x 256\n",
    "    conv3 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu3 = mx.symbol.Activation(\n",
    "        data=conv3, act_type=\"relu\")\n",
    "    # 4. 108 x 256\n",
    "    conv4 = mx.symbol.Convolution(\n",
    "        data=relu3, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu4 = mx.symbol.Activation(\n",
    "        data=conv4, act_type=\"relu\")\n",
    "    # 5. 106 x 256\n",
    "    conv5 = mx.symbol.Convolution(\n",
    "        data=relu4, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu5 = mx.symbol.Activation(\n",
    "        data=conv5, act_type=\"relu\")\n",
    "    # 6. 104 x 256\n",
    "    conv6 = mx.symbol.Convolution(\n",
    "        data=relu5, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu6 = mx.symbol.Activation(\n",
    "        data=conv6, act_type=\"relu\")\n",
    "    pool6 = mx.symbol.Pooling(\n",
    "        data=relu6, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 34 x 256\n",
    "    flatten = mx.symbol.Flatten(data=pool6)\n",
    "    # 7.  8704\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=1024)\n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"relu\")\n",
    "    drop1 = mx.sym.Dropout(act_fc1, p=0.5)\n",
    "    # 8. 1024\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=drop1, num_hidden=1024)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type=\"relu\")\n",
    "    drop2 = mx.sym.Dropout(act_fc2, p=0.5)\n",
    "    # 9. 1024\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=drop2, num_hidden=NOUTPUT)\n",
    "    crepe = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "    return crepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: amazon_review_polarity_train.csv\n",
      "finished processing data frame: amazon_review_polarity_train.csv\n",
      "data contains 2097152 obs\n",
      "('Processed: ', 100000)\n",
      "('Processed: ', 200000)\n",
      "('Processed: ', 300000)\n",
      "('Processed: ', 400000)\n",
      "('Processed: ', 500000)\n",
      "('Processed: ', 600000)\n",
      "('Processed: ', 700000)\n",
      "('Processed: ', 800000)\n",
      "('Processed: ', 900000)\n",
      "('Processed: ', 1000000)\n",
      "('Processed: ', 1100000)\n",
      "('Processed: ', 1200000)\n",
      "('Processed: ', 1300000)\n",
      "('Processed: ', 1400000)\n",
      "('Processed: ', 1500000)\n",
      "('Processed: ', 1600000)\n",
      "('Processed: ', 1700000)\n",
      "('Processed: ', 1800000)\n",
      "('Processed: ', 1900000)\n",
      "('Processed: ', 2000000)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data_frame('amazon_review_polarity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2097152L, 1L, 1014L, 69L)\n",
      "(2097152L,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y\n",
    "# 147 GB, 69% with 2,097,000 mill obs (3.6 will be too much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = create_crepe(), \n",
    "    num_epoch = EPOCHS,  # number of training rounds\n",
    "    learning_rate = 0.01,  # learning rate\n",
    "    momentum = 0.9,   # momentum for sgd\n",
    "    wd = 0.00001,  # weight decay for reg\n",
    "    initializer = mx.init.Normal(sigma=SD)  # init with sd of 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_checkp_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Log*\n",
    "\n",
    "Estimated epoch time = 580 min per epoch\n",
    "\n",
    "`\n",
    "2016-08-23 11:44:46,536 - root - INFO - Start training with [gpu(0)]\n",
    "2016-08-23 11:47:02,052 - root - INFO - Epoch[0] Batch [50]\tSpeed: 5799.10 samples/sec\tTrain-accuracy=0.492344\n",
    "2016-08-23 11:48:50,002 - root - INFO - Epoch[0] Batch [100]\tSpeed: 5929.55 samples/sec\tTrain-accuracy=0.498594\n",
    "2016-08-23 11:50:34,694 - root - INFO - Epoch[0] Batch [150]\tSpeed: 6113.11 samples/sec\tTrain-accuracy=0.509375\n",
    "2016-08-23 11:52:22,282 - root - INFO - Epoch[0] Batch [200]\tSpeed: 5948.62 samples/sec\tTrain-accuracy=0.496250\n",
    "2016-08-23 11:54:09,532 - root - INFO - Epoch[0] Batch [250]\tSpeed: 5967.37 samples/sec\tTrain-accuracy=0.500313\n",
    "2016-08-23 11:55:53,025 - root - INFO - Epoch[0] Batch [300]\tSpeed: 6184.05 samples/sec\tTrain-accuracy=0.505000\n",
    "2016-08-23 11:57:40,069 - root - INFO - Epoch[0] Batch [350]\tSpeed: 5978.85 samples/sec\tTrain-accuracy=0.511563\n",
    "2016-08-23 11:59:25,526 - root - INFO - Epoch[0] Batch [400]\tSpeed: 6068.77 samples/sec\tTrain-accuracy=0.500625\n",
    "2016-08-23 12:01:12,180 - root - INFO - Epoch[0] Batch [450]\tSpeed: 6000.77 samples/sec\tTrain-accuracy=0.502656\n",
    "2016-08-23 12:02:56,980 - root - INFO - Epoch[0] Batch [500]\tSpeed: 6106.81 samples/sec\tTrain-accuracy=0.497500\n",
    "2016-08-23 12:04:42,397 - root - INFO - Epoch[0] Batch [550]\tSpeed: 6071.18 samples/sec\tTrain-accuracy=0.504844\n",
    "2016-08-23 12:06:27,793 - root - INFO - Epoch[0] Batch [600]\tSpeed: 6072.28 samples/sec\tTrain-accuracy=0.485781\n",
    "2016-08-23 12:08:13,898 - root - INFO - Epoch[0] Batch [650]\tSpeed: 6031.82 samples/sec\tTrain-accuracy=0.507188\n",
    "2016-08-23 12:09:58,694 - root - INFO - Epoch[0] Batch [700]\tSpeed: 6107.05 samples/sec\tTrain-accuracy=0.499063\n",
    "2016-08-23 12:11:40,805 - root - INFO - Epoch[0] Batch [750]\tSpeed: 6268.00 samples/sec\tTrain-accuracy=0.491875\n",
    "2016-08-23 12:13:25,710 - root - INFO - Epoch[0] Batch [800]\tSpeed: 6100.70 samples/sec\tTrain-accuracy=0.498594\n",
    "2016-08-23 12:15:10,071 - root - INFO - Epoch[0] Batch [850]\tSpeed: 6132.62 samples/sec\tTrain-accuracy=0.494531\n",
    "2016-08-23 12:16:53,038 - root - INFO - Epoch[0] Batch [900]\tSpeed: 6216.55 samples/sec\tTrain-accuracy=0.497031\n",
    "2016-08-23 12:18:35,365 - root - INFO - Epoch[0] Batch [950]\tSpeed: 6254.46 samples/sec\tTrain-accuracy=0.507031\n",
    "2016-08-23 12:20:19,480 - root - INFO - Epoch[0] Batch [1000]\tSpeed: 6146.99 samples/sec\tTrain-accuracy=0.499063\n",
    "2016-08-23 12:22:04,961 - root - INFO - Epoch[0] Batch [1050]\tSpeed: 6067.73 samples/sec\tTrain-accuracy=0.501094\n",
    "2016-08-23 12:23:51,604 - root - INFO - Epoch[0] Batch [1100]\tSpeed: 6001.67 samples/sec\tTrain-accuracy=0.502500\n",
    "2016-08-23 12:25:35,513 - root - INFO - Epoch[0] Batch [1150]\tSpeed: 6159.24 samples/sec\tTrain-accuracy=0.497188\n",
    "2016-08-23 12:27:19,124 - root - INFO - Epoch[0] Batch [1200]\tSpeed: 6177.90 samples/sec\tTrain-accuracy=0.510781\n",
    "2016-08-23 12:29:04,947 - root - INFO - Epoch[0] Batch [1250]\tSpeed: 6047.78 samples/sec\tTrain-accuracy=0.502344\n",
    "2016-08-23 12:30:48,331 - root - INFO - Epoch[0] Batch [1300]\tSpeed: 6191.47 samples/sec\tTrain-accuracy=0.498750\n",
    "2016-08-23 12:32:33,285 - root - INFO - Epoch[0] Batch [1350]\tSpeed: 6100.87 samples/sec\tTrain-accuracy=0.498125\n",
    "2016-08-23 12:34:18,167 - root - INFO - Epoch[0] Batch [1400]\tSpeed: 6102.10 samples/sec\tTrain-accuracy=0.506563\n",
    "2016-08-23 12:36:05,661 - root - INFO - Epoch[0] Batch [1450]\tSpeed: 5954.65 samples/sec\tTrain-accuracy=0.504375\n",
    "2016-08-23 12:37:49,828 - root - INFO - Epoch[0] Batch [1500]\tSpeed: 6144.22 samples/sec\tTrain-accuracy=0.497812\n",
    "2016-08-23 12:39:36,111 - root - INFO - Epoch[0] Batch [1550]\tSpeed: 6021.60 samples/sec\tTrain-accuracy=0.487031\n",
    "2016-08-23 12:41:20,530 - root - INFO - Epoch[0] Batch [1600]\tSpeed: 6130.09 samples/sec\tTrain-accuracy=0.486250\n",
    "2016-08-23 12:43:04,519 - root - INFO - Epoch[0] Batch [1650]\tSpeed: 6154.62 samples/sec\tTrain-accuracy=0.507500\n",
    "2016-08-23 12:44:50,089 - root - INFO - Epoch[0] Batch [1700]\tSpeed: 6062.27 samples/sec\tTrain-accuracy=0.499063\n",
    "2016-08-23 12:46:36,177 - root - INFO - Epoch[0] Batch [1750]\tSpeed: 6032.78 samples/sec\tTrain-accuracy=0.498437\n",
    "2016-08-23 12:48:20,635 - root - INFO - Epoch[0] Batch [1800]\tSpeed: 6126.81 samples/sec\tTrain-accuracy=0.502188\n",
    "2016-08-23 12:50:06,719 - root - INFO - Epoch[0] Batch [1850]\tSpeed: 6033.01 samples/sec\tTrain-accuracy=0.501250\n",
    "2016-08-23 12:51:52,374 - root - INFO - Epoch[0] Batch [1900]\tSpeed: 6057.68 samples/sec\tTrain-accuracy=0.491250\n",
    "2016-08-23 12:53:37,819 - root - INFO - Epoch[0] Batch [1950]\tSpeed: 6069.46 samples/sec\tTrain-accuracy=0.503594\n",
    "2016-08-23 12:55:23,374 - root - INFO - Epoch[0] Batch [2000]\tSpeed: 6063.25 samples/sec\tTrain-accuracy=0.509375\n",
    "2016-08-23 12:57:08,145 - root - INFO - Epoch[0] Batch [2050]\tSpeed: 6108.50 samples/sec\tTrain-accuracy=0.494688\n",
    "2016-08-23 12:58:54,782 - root - INFO - Epoch[0] Batch [2100]\tSpeed: 6001.73 samples/sec\tTrain-accuracy=0.496875\n",
    "2016-08-23 13:00:41,371 - root - INFO - Epoch[0] Batch [2150]\tSpeed: 6004.32 samples/sec\tTrain-accuracy=0.502188\n",
    "2016-08-23 13:02:27,224 - root - INFO - Epoch[0] Batch [2200]\tSpeed: 6046.12 samples/sec\tTrain-accuracy=0.503906\n",
    "2016-08-23 13:04:12,967 - root - INFO - Epoch[0] Batch [2250]\tSpeed: 6052.41 samples/sec\tTrain-accuracy=0.497031\n",
    "2016-08-23 13:05:57,377 - root - INFO - Epoch[0] Batch [2300]\tSpeed: 6129.74 samples/sec\tTrain-accuracy=0.498594\n",
    "2016-08-23 13:07:41,209 - root - INFO - Epoch[0] Batch [2350]\tSpeed: 6163.80 samples/sec\tTrain-accuracy=0.497188\n",
    "2016-08-23 13:09:29,085 - root - INFO - Epoch[0] Batch [2400]\tSpeed: 5933.62 samples/sec\tTrain-accuracy=0.505313\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
