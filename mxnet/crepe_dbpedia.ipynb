{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]\n",
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128*8\n",
    "NUM_FILTERS = 256\n",
    "EPOCHS = 10000\n",
    "SD = 0.05  # std for gaussian distribution\n",
    "NOUTPUT = 14  # good or bad\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='crepe_inram_onegpu.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        # print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('\\nsaved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_frame(infile, shuffle = False):\n",
    "    print(\"processing data frame: %s\" % infile)\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "        # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'])\n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='bool')\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET), dtype='bool'), columns=ALPHABET)\n",
    "    print(\"finished processing data frame: %s\" % infile)\n",
    "    print(\"data contains %d obs\" % df.shape[0])\n",
    "    batch_size = df.shape[0]\n",
    "    # Create encoding\n",
    "    X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='bool')\n",
    "    # Main loop\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if (ti+1) % (100*1000) == 0:\n",
    "            print(\"Processed: \", ti+1)\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch], dtype='bool')\n",
    "                \n",
    "    # Return as a DataBatch\n",
    "    #return DataBatch(data=[mx.nd.array(X_split)],\n",
    "    #                 label=[mx.nd.array(y_split[ti + 1 - batch_size:ti + 1])])\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_crepe():\n",
    "    \"\"\"\n",
    "    Replicating: https://github.com/zhangxiangxiao/Crepe/blob/master/train/config.lua\n",
    "    \"\"\"\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "    # 1. alphabet x 1014\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=input_x, kernel=(7, 69), num_filter=NUM_FILTERS)\n",
    "    relu1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"relu\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=relu1, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 2. 336 x 256\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(7, 1), num_filter=NUM_FILTERS)\n",
    "    relu2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"relu\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=relu2, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 3. 110 x 256\n",
    "    conv3 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu3 = mx.symbol.Activation(\n",
    "        data=conv3, act_type=\"relu\")\n",
    "    # 4. 108 x 256\n",
    "    conv4 = mx.symbol.Convolution(\n",
    "        data=relu3, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu4 = mx.symbol.Activation(\n",
    "        data=conv4, act_type=\"relu\")\n",
    "    # 5. 106 x 256\n",
    "    conv5 = mx.symbol.Convolution(\n",
    "        data=relu4, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu5 = mx.symbol.Activation(\n",
    "        data=conv5, act_type=\"relu\")\n",
    "    # 6. 104 x 256\n",
    "    conv6 = mx.symbol.Convolution(\n",
    "        data=relu5, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu6 = mx.symbol.Activation(\n",
    "        data=conv6, act_type=\"relu\")\n",
    "    pool6 = mx.symbol.Pooling(\n",
    "        data=relu6, pool_type=\"max\", kernel=(3, 1), stride=(1, 1))\n",
    "    # 34 x 256\n",
    "    flatten = mx.symbol.Flatten(data=pool6)\n",
    "    # 7.  8704\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=1024)\n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"relu\")\n",
    "    drop1 = mx.sym.Dropout(act_fc1, p=0.5)\n",
    "    # 8. 1024\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=drop1, num_hidden=1024)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type=\"relu\")\n",
    "    drop2 = mx.sym.Dropout(act_fc2, p=0.5)\n",
    "    # 9. 1024\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=drop2, num_hidden=NOUTPUT)\n",
    "    crepe = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "    return crepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: dbpedia_train.csv\n",
      "downloading ...\n",
      "\n",
      "100% [......................................................................] 174148970 / 174148970\n",
      "saved data\n",
      "finished processing data frame: dbpedia_train.csv\n",
      "data contains 560000 obs\n",
      "('Processed: ', 100000)\n",
      "('Processed: ', 200000)\n",
      "('Processed: ', 300000)\n",
      "('Processed: ', 400000)\n",
      "('Processed: ', 500000)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data_frame('dbpedia_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: dbpedia_test.csv\n",
      "downloading ...\n",
      "\n",
      "100% [........................................................................] 21775285 / 21775285\n",
      "saved data\n",
      "finished processing data frame: dbpedia_test.csv\n",
      "data contains 70000 obs\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = load_data_frame('dbpedia_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000L, 1L, 1014L, 69L)\n",
      "(560000L,)\n",
      "(70000L, 1L, 1014L, 69L)\n",
      "(70000L,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_iter = mx.io.NDArrayIter(test_x, test_y, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = create_crepe(), \n",
    "    num_epoch = EPOCHS,  # number of training rounds\n",
    "    learning_rate = 0.01,  # learning rate\n",
    "    momentum = 0.9,   # momentum for sgd\n",
    "    wd = 0.00001,  # weight decay for reg\n",
    "    initializer = mx.init.Normal(sigma=SD)  # init with sd of 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    eval_data = test_iter,\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_check_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))\n",
    "\n",
    "\"\"\"\n",
    "50 batches take 6 mins\n",
    "We have 560,000 / (128*8) = 546 batches\n",
    "Epoch should take 66 min\n",
    "\n",
    "2016-08-24 13:29:24,227 - root - INFO - Start training with [gpu(0), gpu(1), gpu(2), gpu(3)]\n",
    "2016-08-24 13:35:49,740 - root - INFO - Epoch[0] Batch [50]\tSpeed: 14194.27 samples/sec\tTrain-accuracy=0.895391\n",
    "2016-08-24 13:41:49,545 - root - INFO - Epoch[0] Batch [100]\tSpeed: 14229.97 samples/sec\tTrain-accuracy=0.927656\n",
    "2016-08-24 13:48:00,082 - root - INFO - Epoch[0] Batch [150]\tSpeed: 13817.75 samples/sec\tTrain-accuracy=0.930039\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
