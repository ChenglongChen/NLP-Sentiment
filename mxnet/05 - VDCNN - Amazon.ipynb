{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSUMMARY:\\nAmazon pos/neg sentiment classification\\n\\nAccuracy: 0.94\\nTime per Epoch: 9550 seconds = 220 rev/s\\nTotal time: 9550*10 = 1592 min = 26.5 hours\\nTrain size = 2,097,152\\nTest size = 233,016\\n\\nDETAILS:\\nAttempt to replicate crepe model using MXNET:\\nhttps://github.com/zhangxiangxiao/Crepe\\n\\nThis uses an efficient numpy array (dtype=bool)\\nto hold all data in RAM. \\n\\nRun on one GPU (Tesla K80) with batch=128\\nPeak RAM: 142GB, and training cut to: 2,097,152 (from 3.6M)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SUMMARY:\n",
    "Amazon pos/neg sentiment classification\n",
    "\n",
    "TBD\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128\n",
    "NUM_FILTERS = 256\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))\n",
    "\n",
    "ctx = mx.cpu()\n",
    "EPOCHS = 10\n",
    "SD = 0.05  # std for gaussian distribution\n",
    "NOUTPUT = 2  # good or bad\n",
    "INITY = mx.init.Normal(sigma=SD)\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='crepe_amazon.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        # print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('\\nsaved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_frame(infile, shuffle = False):\n",
    "    print(\"processing data frame: %s\" % infile)\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "    \n",
    "    # 3.6 mill is too much, use 2 mill (keep same ratio)\n",
    "    if \"test\" in infile:\n",
    "        maxrows = int(2097152/9)  # 16,384 batches\n",
    "    elif \"train\" in infile:\n",
    "        maxrows = int(2097152)\n",
    "\n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'],\n",
    "                     nrows=maxrows)\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    \n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='bool')\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET), dtype='bool'), columns=ALPHABET)\n",
    "    print(\"finished processing data frame: %s\" % infile)\n",
    "    print(\"data contains %d obs\" % df.shape[0])\n",
    "    batch_size = df.shape[0]\n",
    "    # Create encoding\n",
    "    X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='bool')\n",
    "    # Main loop\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if (ti+1) % (100*1000) == 0:\n",
    "            print(\"Processed: \", ti+1)\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch], dtype='bool')\n",
    "                \n",
    "    # Return as a DataBatch\n",
    "    #return DataBatch(data=[mx.nd.array(X_split)],\n",
    "    #                 label=[mx.nd.array(y_split[ti + 1 - batch_size:ti + 1])])\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vdcnn():\n",
    "    \"\"\"\n",
    "    Replicating: ...\n",
    "    \"\"\"  \n",
    "    \n",
    "    vocab_size = 69\n",
    "    embedding_size = 16\n",
    "    kernel = (3, embedding_size)\n",
    "    stride = (2, embedding_size)\n",
    "    num_filters1 = 64\n",
    "    num_filters2 = num_filters1 * 2\n",
    "    num_filters3 = num_filters1 * 3\n",
    "    num_filters4 = num_filters1 * 4\n",
    "\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output  \n",
    "\n",
    "    # Lookup Table 16\n",
    "    embed_layer = mx.symbol.Embedding(\n",
    "        data = input_x, input_dim = vocab_size, output_dim = embedding_size, name = 'word_embedding')\n",
    "    conv_input = mx.symbol.Reshape(\n",
    "        data = embed_layer, shape = (BATCH_SIZE, 1, FEATURE_LEN, embedding_size))\n",
    "        \n",
    "    # Temp Conv\n",
    "    conv0 = mx.symbol.Convolution(\n",
    "        data=conv_input, num_filter=num_filters1, kernel=kernel)\n",
    "\n",
    "    # CONVOLUTION_BLOCK (1 of 4) -> 64 FILTERS\n",
    "    conv11 = mx.symbol.Convolution(\n",
    "        data=conv0, kernel=kernel, num_filter=num_filters1)\n",
    "    norm11 = mx.symbol.BatchNorm(\n",
    "        data=conv11)\n",
    "    act11 = mx.symbol.Activation(\n",
    "        data=norm11, act_type='relu')\n",
    "    conv12 = mx.symbol.Convolution(\n",
    "        data=act11, kernel=kernel, num_filter=num_filters1)\n",
    "    norm12 = mx.symbol.BatchNorm(\n",
    "        data=conv12)\n",
    "    act12 = mx.symbol.Activation(\n",
    "        data=norm12, act_type='relu')\n",
    "    conv21 = mx.symbol.Convolution(\n",
    "        data=act12, kernel=kernel, num_filter=num_filters1)\n",
    "    norm21 = mx.symbol.BatchNorm(\n",
    "        data=conv21)\n",
    "    act21 = mx.symbol.Activation(\n",
    "        data=norm21, act_type='relu')\n",
    "    conv22 = mx.symbol.Convolution(\n",
    "        data=act21, kernel=kernel, num_filter=num_filters1)\n",
    "    norm22 = mx.symbol.BatchNorm(\n",
    "        data=conv22)\n",
    "    act22 = mx.symbol.Activation(\n",
    "        data=norm22, act_type='relu')\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=act22, pool_type='max', kernel=kernel, stride=stride)\n",
    "\n",
    "    # CONVOLUTION_BLOCK (2 of 4) -> 128 FILTERS\n",
    "    conv31 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=kernel, num_filter=num_filters2)\n",
    "    norm31 = mx.symbol.BatchNorm(\n",
    "        data=conv31)\n",
    "    act31 = mx.symbol.Activation(\n",
    "        data=norm31, act_type='relu')\n",
    "    conv32 = mx.symbol.Convolution(\n",
    "        data=act31, kernel=kernel, num_filter=num_filters2)\n",
    "    norm32 = mx.symbol.BatchNorm(\n",
    "        data=conv32)\n",
    "    act32 = mx.symbol.Activation(\n",
    "        data=norm32, act_type='relu')\n",
    "    conv41 = mx.symbol.Convolution(\n",
    "        data=act32, kernel=kernel, num_filter=num_filters2)\n",
    "    norm41 = mx.symbol.BatchNorm(\n",
    "        data=conv41)\n",
    "    act41 = mx.symbol.Activation(\n",
    "        data=norm41, act_type='relu')\n",
    "    conv42 = mx.symbol.Convolution(\n",
    "        data=act41, kernel=kernel, num_filter=num_filters2)\n",
    "    norm42 = mx.symbol.BatchNorm(\n",
    "        data=conv42)\n",
    "    act42 = mx.symbol.Activation(\n",
    "        data=norm42, act_type='relu')\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=act42, pool_type='max', kernel=kernel, stride=stride)\n",
    "\n",
    "     # CONVOLUTION_BLOCK (3 of 4) -> 256 FILTERS\n",
    "    conv51 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=kernel, num_filter=num_filters3)\n",
    "    norm51 = mx.symbol.BatchNorm(\n",
    "        data=conv51)\n",
    "    act51 = mx.symbol.Activation(\n",
    "        data=norm51, act_type='relu')\n",
    "    conv52 = mx.symbol.Convolution(\n",
    "        data=act51, kernel=kernel, num_filter=num_filters3)\n",
    "    norm52 = mx.symbol.BatchNorm(\n",
    "        data=conv52)\n",
    "    act52 = mx.symbol.Activation(\n",
    "        data=norm52, act_type='relu')\n",
    "    conv61 = mx.symbol.Convolution(\n",
    "        data=act52, kernel=kernel, num_filter=num_filters3)\n",
    "    norm61 = mx.symbol.BatchNorm(\n",
    "        data=conv61)\n",
    "    act61 = mx.symbol.Activation(\n",
    "        data=norm61, act_type='relu')\n",
    "    conv62 = mx.symbol.Convolution(\n",
    "        data=act61, kernel=kernel, num_filter=num_filters3)\n",
    "    norm62 = mx.symbol.BatchNorm(\n",
    "        data=conv62)\n",
    "    act62 = mx.symbol.Activation(\n",
    "        data=norm62, act_type='relu')\n",
    "    pool3 = mx.symbol.Pooling(\n",
    "        data=act62, pool_type='max', kernel=kernel, stride=stride)   \n",
    "\n",
    "     # CONVOLUTION_BLOCK (4 of 4) -> 512 FILTERS\n",
    "    conv71 = mx.symbol.Convolution(\n",
    "        data=pool3, kernel=kernel, num_filter=num_filters4)\n",
    "    norm71 = mx.symbol.BatchNorm(\n",
    "        data=conv71)\n",
    "    act71 = mx.symbol.Activation(\n",
    "        data=norm71, act_type='relu')\n",
    "    conv72 = mx.symbol.Convolution(\n",
    "        data=act71, kernel=kernel, num_filter=num_filters4)\n",
    "    norm72 = mx.symbol.BatchNorm(\n",
    "        data=conv72)\n",
    "    act72 = mx.symbol.Activation(\n",
    "        data=norm72, act_type='relu')\n",
    "    conv81 = mx.symbol.Convolution(\n",
    "        data=act72, kernel=kernel, num_filter=num_filters4)\n",
    "    norm81 = mx.symbol.BatchNorm(\n",
    "        data=conv81)\n",
    "    act81 = mx.symbol.Activation(\n",
    "        data=norm81, act_type='relu')\n",
    "    conv82 = mx.symbol.Convolution(\n",
    "        data=act81, kernel=kernel, num_filter=num_filters4)\n",
    "    norm82 = mx.symbol.BatchNorm(\n",
    "        data=conv82)\n",
    "    act82 = mx.symbol.Activation(\n",
    "        data=norm82, act_type='relu')\n",
    "    pool4 = mx.symbol.Pooling(\n",
    "        data=act82, pool_type='max', kernel=kernel, stride=stride) \n",
    "\n",
    "    # Flatten (dimensions * feature length * filters)\n",
    "    flatten = mx.symbol.Flatten(data=pool4)\n",
    "\n",
    "    # First fully connected\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=2048) \n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type='relu')\n",
    "    # Second fully connected\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=act_fc1, num_hidden=2048)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type='relu')\n",
    "    # Third fully connected\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=act_fc2, num_hidden=NOUTPUT)\n",
    "    net = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise symbol (for crepe)\n",
    "cnn = create_vdcnn()\n",
    "\n",
    "a = mx.viz.plot_network(cnn)\n",
    "a.render('Crepe Model')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: amazon_review_polarity_train.csv\n",
      "finished processing data frame: amazon_review_polarity_train.csv\n",
      "data contains 2097152 obs\n",
      "Processed:  100000\n",
      "Processed:  200000\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data_frame('amazon_review_polarity_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = cnn, \n",
    "    num_epoch = EPOCHS,  # number of training rounds\n",
    "    learning_rate = LR,  # learning rate\n",
    "    momentum = MOMENTUM,   # momentum for sgd\n",
    "    wd = WDECAY,  # weight decay for reg\n",
    "    initializer = INITY  # init with sd of 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"vdcnn_checkp_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# GPU broke after completing 7th epoch\n",
    "# Re-load checkpoint\n",
    "# If training breaks (happens on GPU), we can train further like so:\n",
    "\n",
    "# Load trained model:\n",
    "pretrained_model = mx.model.FeedForward.load(\"crepe_checkp_v2_\", 7)  \n",
    "\n",
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol=pretrained_model.symbol,\n",
    "    arg_params=pretrained_model.arg_params,\n",
    "    aux_params=pretrained_model.aux_params,\n",
    "    num_epoch=11, begin_epoch=7\n",
    ")\n",
    "\n",
    "# Train remaining epochs\n",
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_checkp_v2_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_x, test_y = load_data_frame('amazon_review_polarity_test.csv')\n",
    "test_iter = mx.io.NDArrayIter(test_x, test_y, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = np.argsort(model.predict(X = test_iter))[:,-1]\n",
    "\n",
    "# Save Results\n",
    "np.savetxt('crepe_predict_sentiment_amazon.csv', np.c_[pred, test_y], delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94166495004634876"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = sum(pred==test_y.astype('int'))/float(len(test_y))\n",
    "logger.info(acc)\n",
    "acc  # 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Extract\n",
    "\n",
    "```\n",
    "2016-08-24 21:11:28,407 - root - INFO - Start training with [gpu(0)]\n",
    "2016-08-24 21:12:40,015 - root - INFO - Epoch[0] Batch [50]\tSpeed: 11477.97 samples/sec\tTrain-accuracy=0.502031\n",
    "2016-08-24 21:13:15,326 - root - INFO - Epoch[0] Batch [100]\tSpeed: 18131.85 samples/sec\tTrain-accuracy=0.514531\n",
    "2016-08-24 21:13:46,977 - root - INFO - Epoch[0] Batch [150]\tSpeed: 20220.53 samples/sec\tTrain-accuracy=0.515938\n",
    "2016-08-24 21:14:17,447 - root - INFO - Epoch[0] Batch [200]\tSpeed: 21004.96 samples/sec\tTrain-accuracy=0.522344\n",
    "2016-08-24 21:14:48,170 - root - INFO - Epoch[0] Batch [250]\tSpeed: 20831.30 samples/sec\tTrain-accuracy=0.540156\n",
    "2016-08-24 21:15:19,698 - root - INFO - Epoch[0] Batch [300]\tSpeed: 20298.77 samples/sec\tTrain-accuracy=0.524062\n",
    "2016-08-24 21:15:50,674 - root - INFO - Epoch[0] Batch [350]\tSpeed: 20661.82 samples/sec\tTrain-accuracy=0.522344\n",
    "2016-08-24 21:16:21,581 - root - INFO - Epoch[0] Batch [400]\tSpeed: 20718.01 samples/sec\tTrain-accuracy=0.526406\n",
    "2016-08-24 21:16:53,575 - root - INFO - Epoch[0] Batch [450]\tSpeed: 20003.13 samples/sec\tTrain-accuracy=0.522969\n",
    "2016-08-24 21:17:24,561 - root - INFO - Epoch[0] Batch [500]\tSpeed: 20655.16 samples/sec\tTrain-accuracy=0.529375\n",
    "2016-08-24 21:17:55,624 - root - INFO - Epoch[0] Batch [550]\tSpeed: 20603.29 samples/sec\tTrain-accuracy=0.538125\n",
    "2016-08-24 21:18:26,703 - root - INFO - Epoch[0] Batch [600]\tSpeed: 20592.68 samples/sec\tTrain-accuracy=0.540312\n",
    "2016-08-24 21:18:58,474 - root - INFO - Epoch[0] Batch [650]\tSpeed: 20143.52 samples/sec\tTrain-accuracy=0.541406\n",
    "2016-08-24 21:19:30,115 - root - INFO - Epoch[0] Batch [700]\tSpeed: 20226.92 samples/sec\tTrain-accuracy=0.544063\n",
    "2016-08-24 21:20:00,844 - root - INFO - Epoch[0] Batch [750]\tSpeed: 20838.08 samples/sec\tTrain-accuracy=0.544219\n",
    "2016-08-24 21:20:30,993 - root - INFO - Epoch[0] Batch [800]\tSpeed: 21239.17 samples/sec\tTrain-accuracy=0.544375\n",
    "2016-08-24 21:21:02,349 - root - INFO - Epoch[0] Batch [850]\tSpeed: 20419.88 samples/sec\tTrain-accuracy=0.573125\n",
    "2016-08-24 21:21:33,382 - root - INFO - Epoch[0] Batch [900]\tSpeed: 20623.21 samples/sec\tTrain-accuracy=0.595625\n",
    "2016-08-24 21:22:03,338 - root - INFO - Epoch[0] Batch [950]\tSpeed: 21365.38 samples/sec\tTrain-accuracy=0.521875\n",
    "2016-08-24 21:22:36,104 - root - INFO - Epoch[0] Batch [1000]\tSpeed: 19541.98 samples/sec\tTrain-accuracy=0.525156\n",
    "...\n",
    "2016-08-26 20:47:59,068 - root - INFO - Epoch[10] Batch [15650]\tSpeed: 22745.04 samples/sec\tTrain-accuracy=0.973437\n",
    "2016-08-26 20:48:27,661 - root - INFO - Epoch[10] Batch [15700]\tSpeed: 22383.10 samples/sec\tTrain-accuracy=0.973437\n",
    "2016-08-26 20:48:56,799 - root - INFO - Epoch[10] Batch [15750]\tSpeed: 21964.45 samples/sec\tTrain-accuracy=0.969375\n",
    "2016-08-26 20:49:25,234 - root - INFO - Epoch[10] Batch [15800]\tSpeed: 22506.68 samples/sec\tTrain-accuracy=0.968594\n",
    "2016-08-26 20:49:54,374 - root - INFO - Epoch[10] Batch [15850]\tSpeed: 21963.69 samples/sec\tTrain-accuracy=0.973594\n",
    "2016-08-26 20:50:22,418 - root - INFO - Epoch[10] Batch [15900]\tSpeed: 22820.47 samples/sec\tTrain-accuracy=0.969531\n",
    "2016-08-26 20:50:50,575 - root - INFO - Epoch[10] Batch [15950]\tSpeed: 22729.69 samples/sec\tTrain-accuracy=0.972969\n",
    "2016-08-26 20:51:19,290 - root - INFO - Epoch[10] Batch [16000]\tSpeed: 22288.00 samples/sec\tTrain-accuracy=0.970000\n",
    "2016-08-26 20:51:48,007 - root - INFO - Epoch[10] Batch [16050]\tSpeed: 22286.45 samples/sec\tTrain-accuracy=0.972812\n",
    "2016-08-26 20:52:15,960 - root - INFO - Epoch[10] Batch [16100]\tSpeed: 22896.39 samples/sec\tTrain-accuracy=0.971875\n",
    "2016-08-26 20:52:43,770 - root - INFO - Epoch[10] Batch [16150]\tSpeed: 23012.48 samples/sec\tTrain-accuracy=0.970938\n",
    "2016-08-26 20:53:13,394 - root - INFO - Epoch[10] Batch [16200]\tSpeed: 21615.78 samples/sec\tTrain-accuracy=0.967656\n",
    "2016-08-26 20:53:43,127 - root - INFO - Epoch[10] Batch [16250]\tSpeed: 21524.90 samples/sec\tTrain-accuracy=0.970313\n",
    "2016-08-26 20:54:11,734 - root - INFO - Epoch[10] Batch [16300]\tSpeed: 22371.36 samples/sec\tTrain-accuracy=0.967812\n",
    "2016-08-26 20:54:40,546 - root - INFO - Epoch[10] Batch [16350]\tSpeed: 22213.74 samples/sec\tTrain-accuracy=0.971250\n",
    "2016-08-26 20:55:00,513 - root - INFO - Epoch[10] Resetting Data Iterator\n",
    "2016-08-26 20:55:00,513 - root - INFO - Epoch[10] Time cost=9474.580\n",
    "2016-08-26 21:16:02,765 - root - INFO - 0.941664950046\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}