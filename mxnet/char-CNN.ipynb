{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://arxiv.org/abs/1509.01626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet 69 characters:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', ',', ';', '.', '!', '?', ':', \"'\", '\"', '/', '\\\\', '|', '_', '@', '#', '$', '%', '^', '&', '*', '~', '`', '+', ' ', '=', '<', '>', '(', ')', '[', ']', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "print(\"Alphabet %d characters: \" % len(ALPHABET), ALPHABET)\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128\n",
    "NUM_FILTERS = 256\n",
    "NUM_EPOCHS = 30  # config.main.epoches = 5000\n",
    "SD = 0.05  # stdev for gaussian distribution\n",
    "NOUTPUT = 2  # good or bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        #print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('saved data\\n')\n",
    "\n",
    "\n",
    "def load_data_frame(infile, batch_size=128, shuffle=True):\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "\n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'])\n",
    "\n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='int')\n",
    "    # print(Y_split[:30])\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET)), columns=ALPHABET)\n",
    "    # Yield mini-batch amount of character vectors\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if ti % batch_size == 0:\n",
    "            # output\n",
    "            if ti > 0:\n",
    "                yield X_split, y_split[ti - batch_size:ti]\n",
    "            X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='int')\n",
    "\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch])\n",
    "\n",
    "                \n",
    "def example():\n",
    "    count = 0\n",
    "    for minibatch in load_data_frame('amazon_review_polarity_test.csv', batch_size=5, shuffle=True):\n",
    "        count += 1\n",
    "        print(minibatch[-1])\n",
    "        if count == 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1]\n",
      "[0 0 0 1 0]\n",
      "[1 0 0 1 0]\n",
      "[0 1 1 1 1]\n",
      "[0 0 1 1 0]\n",
      "[0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_crepe():\n",
    "    \"\"\"\n",
    "    Number of features = 70, input feature length = 1014\n",
    "    2 Dropout modules inserted between 3 fully-connected layers (0.5)\n",
    "    Number of output units for last layer = num_classes\n",
    "    For polarity test = 2\n",
    "    \n",
    "    Replicating: https://github.com/zhangxiangxiao/Crepe/blob/master/train/config.lua\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    input_x = mx.sym.Variable('data')# placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label') # placeholder for output\n",
    "    \n",
    "    # 6 Convolutional layers\n",
    "    \n",
    "    # 1. alphabet x 1014\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=input_x, kernel=(7, 7), num_filter=NUM_FILTERS)\n",
    "    relu1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"relu\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=relu1, pool_type=\"max\", kernel=(3, 3), stride=(1, 1))\n",
    "    \n",
    "    # 2. 336 x 256\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(7, 7), num_filter=NUM_FILTERS)\n",
    "    relu2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"relu\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=relu2, pool_type=\"max\", kernel=(3, 3), stride=(1, 1))\n",
    "    \n",
    "    # 3. 110 x 256\n",
    "    conv3 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=(3, 3), num_filter=NUM_FILTERS)\n",
    "    relu3 = mx.symbol.Activation(\n",
    "        data=conv3, act_type=\"relu\")\n",
    "    \n",
    "    # 4. 108 x 256\n",
    "    conv4 = mx.symbol.Convolution(\n",
    "        data=relu3, kernel=(3, 3), num_filter=NUM_FILTERS)\n",
    "    relu4 = mx.symbol.Activation(\n",
    "        data=conv4, act_type=\"relu\")\n",
    "    \n",
    "    # 5. 106 x 256\n",
    "    conv5 = mx.symbol.Convolution(\n",
    "        data=relu4, kernel=(3, 3), num_filter=NUM_FILTERS)\n",
    "    relu5 = mx.symbol.Activation(\n",
    "        data=conv5, act_type=\"relu\")\n",
    "    \n",
    "    # 6. 104 x 256\n",
    "    conv6 = mx.symbol.Convolution(\n",
    "        data=relu5, kernel=(3, 3), num_filter=NUM_FILTERS)\n",
    "    relu6 = mx.symbol.Activation(\n",
    "        data=conv6, act_type=\"relu\")\n",
    "    pool6 = mx.symbol.Pooling(\n",
    "        data=relu6, pool_type=\"max\", kernel=(3, 3), stride=(1, 1))\n",
    "    \n",
    "    # 34 x 256\n",
    "    flatten = mx.symbol.Flatten(data=pool6)\n",
    "    \n",
    "    # 3 Fully-connected layers\n",
    "    \n",
    "    # 7.  8704\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=1024)\n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"relu\")\n",
    "    drop1 = mx.sym.Dropout(act_fc1, p=0.5)\n",
    "    \n",
    "    # 8. 1024\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=drop1, num_hidden=1024)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type=\"relu\")\n",
    "    drop2 = mx.sym.Dropout(act_fc2, p=0.5)\n",
    "    \n",
    "    # 9. 1024\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=drop2, num_hidden=NOUTPUT)\n",
    "    crepe = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "\n",
    "    return crepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_model():\n",
    "\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "\n",
    "    num_label = 2\n",
    "    filter_list = [3, 4, 5]\n",
    "    num_filter = 100\n",
    "\n",
    "    # create convolution + (max) pooling layer for each filter operation\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        convi = mx.sym.Convolution(\n",
    "            data=input_x, kernel=(filter_size, len(ALPHABET)), num_filter=num_filter)\n",
    "        relui = mx.sym.Activation(\n",
    "            data=convi, act_type='relu')\n",
    "        pooli = mx.sym.Pooling(\n",
    "            data=relui, pool_type='max', kernel=(FEATURE_LEN - filter_size + 1, 1), stride=(1, 1))\n",
    "        pooled_outputs.append(pooli)\n",
    "\n",
    "    # combine all pooled outputs\n",
    "    total_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "    h_pool = mx.sym.Reshape(data=concat, target_shape=(BATCH_SIZE, total_filters))\n",
    "\n",
    "    # dropout layer\n",
    "    dropout = 0.5\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "\n",
    "    # fully connected\n",
    "    cls_weight = mx.sym.Variable('cls_weight')\n",
    "    cls_bias = mx.sym.Variable('cls_bias')\n",
    "    fc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "    # softmax output\n",
    "    model = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn = simple_model()\n",
    "cnn = simple_model()\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crepe.pdf'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise symbol (for crepe)\n",
    "a = mx.viz.plot_network(create_crepe())\n",
    "a.render('crepe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# The mean and standard deviation used for\n",
    "# initializing the large model is (0, 0.02) and small model (0, 0.05).\n",
    "initializer = mx.initializer.Normal(sigma=SD)\n",
    "\n",
    "# Create arguments\n",
    "arg_names = cnn.list_arguments()\n",
    "input_shapes = {'data': (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))}\n",
    "shapey = cnn.infer_shape(**input_shapes)\n",
    "arg_shape, out_shape, aux_shape = shapey\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']:  # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "# Bind symbol current symbol to get an executor\n",
    "cnn_exec = cnn.bind(ctx=ctx,\n",
    "                    args=arg_arrays,\n",
    "                    args_grad=args_grad,\n",
    "                    grad_req='add')\n",
    "\n",
    "\n",
    "# Group symbols and create executor\n",
    "#cnn_exec = cnn.simple_bind(ctx=ctx, grad_req='add', **input_shapes)\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']:  # input, output\n",
    "        continue\n",
    "    initializer(name, arg_dict[name])\n",
    "\n",
    "    param_blocks.append((i, arg_dict[name], args_grad[name], name))\n",
    "\n",
    "out_dict = dict(zip(cnn.list_outputs(), cnn_exec.outputs))\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "m = CNNModel(cnn_exec=cnn_exec,\n",
    "             symbol=cnn,\n",
    "             data=data, \n",
    "             label=label,\n",
    "             param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "opt = mx.optimizer.create('SGD')\n",
    "opt.learning_rate  = 0.01\n",
    "opt.momentum = 0.9\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "for iteration in range(NUM_EPOCHS):\n",
    "    \n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for batchX, batchY in load_data_frame('amazon_review_polarity_test.csv', BATCH_SIZE):\n",
    "\n",
    "        m.data[:] = batchX\n",
    "        m.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        m.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(m.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "        # every 12,800\n",
    "        if num_total % (BATCH_SIZE*100) == 0:\n",
    "            print(\"Processed %d\" % num_total)\n",
    "            \n",
    "        # update weights\n",
    "        # executor weights, biases and parameters\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in m.param_blocks:\n",
    "            grad /= BATCH_SIZE\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in m.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # decay learning rate\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "            \n",
    "    # end of training loop\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f' % (iteration, train_time, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
