{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    Downloads a file from a url if the file does not exist in the current folder\n",
    "    :param url: Url to the file\n",
    "    \"\"\"\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        print(\"The file %s already exist in the current directory\" % local_filename)\n",
    "    else:\n",
    "        print('downloading data: %s' % url)\n",
    "        response = wget.download(url)\n",
    "        print('saved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file https://hoaphumanoidstorage2.blob.core.windows.net/public/mnist_train.csv\n",
      "The file mnist_train.csv already exist in the current directory\n",
      "Downloading file https://hoaphumanoidstorage2.blob.core.windows.net/public/mnist_test.csv\n",
      "The file mnist_test.csv already exist in the current directory\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "url_train = 'https://hoaphumanoidstorage2.blob.core.windows.net/public/mnist_train.csv'\n",
    "url_test = 'https://hoaphumanoidstorage2.blob.core.windows.net/public/mnist_test.csv'\n",
    "\n",
    "print(\"Downloading file %s\" % url_train)\n",
    "download_file(url_train)\n",
    "\n",
    "print(\"Downloading file %s\" % url_test)\n",
    "download_file(url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://yann.lecun.com/exdb/mnist/\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import time\n",
    "import math\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, 28, 28)\n",
    "EPOCHS = 10\n",
    "LR  = 0.07\n",
    "MOM = 0.9\n",
    "WD = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='lenet.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gather data\n",
    "train = pd.read_csv('mnist_train.csv', header=None)\n",
    "train_y = train[[0]].values.ravel()\n",
    "train_x = train.iloc[:,1:].values\n",
    "\n",
    "# modify data\n",
    "train_x = np.array(train_x, dtype='float32').reshape((-1, 1, 28, 28))\n",
    "#print(train_x.shape)  # (60000, 1, 28, 28)\n",
    "# normalise (between 0 and 1)\n",
    "train_x[:] /= 255.0\n",
    "\n",
    "# iterator to feed mini_batch at a time\n",
    "# returns <mxnet.io.DataBatch object at 0x000001AA996B38D0> \n",
    "# type <class 'mxnet.io.DataBatch'>\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "def create_lenet():\n",
    "    # create symbolic representation\n",
    "    data = mx.symbol.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=data, kernel=(5,5), num_filter=20)\n",
    "    tanh1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(5,5), num_filter=50)\n",
    "    tanh2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2)) \n",
    "\n",
    "    flatten = mx.symbol.Flatten(\n",
    "        data=pool2)\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=500) \n",
    "    tanh3 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"tanh\")\n",
    "\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=tanh3, num_hidden=10) \n",
    "\n",
    "    lenet = mx.symbol.SoftmaxOutput(\n",
    "        data=fc2, label=input_y, name=\"softmax\")\n",
    "    return lenet\n",
    "\n",
    "# train the NN\n",
    "ctx = mx.cpu()\n",
    "cnn = create_lenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lenet.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise symbol (for crepe)\n",
    "a = mx.viz.plot_network(cnn)\n",
    "a.render('lenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method A - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = cnn, \n",
    "    num_epoch = EPOCHS,\n",
    "    learning_rate = LR,\n",
    "    momentum = MOM, \n",
    "    wd = WD\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log accuracy to file every batch\n",
    "# Save parameters at every epoch\n",
    "\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"lenet_checkp_\") \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction of test set\n",
    "test = pd.read_csv('mnist_test.csv', header=None)\n",
    "test_y = test[[0]].values.ravel()\n",
    "test_x = test.iloc[:,1:].values\n",
    "\n",
    "test_x = np.array(test_x, dtype='float32').reshape((-1, 1, 28, 28))\n",
    "test_x[:] /= 255.0\n",
    "\n",
    "test_iter = mx.io.NDArrayIter(test_x, test_y, batch_size=100)\n",
    "\n",
    "# most likely will be last element after sorting\n",
    "pred = np.argsort(model.predict(X = test_iter))[:,-1]\n",
    "\n",
    "# accuracy\n",
    "print(sum(pred==test_y)/len(test_y))\n",
    "\n",
    "# save\n",
    "np.savetxt('predicted_images.csv', np.c_[pred, test_y], delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method B - Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create own iterator (that can optionally apply transform function)\n",
    "def manual_iterator(infile, y_split, batch_size=100):\n",
    "    for ti, tx in enumerate(infile):\n",
    "            if ti % batch_size == 0:\n",
    "                # output\n",
    "                if ti > 0:\n",
    "                    yield X_split, y_split[ti-batch_size:ti]\n",
    "                X_split = np.zeros(DATA_SHAPE, dtype='float32')\n",
    "            X_split[ti%batch_size][0] = tx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup executor\n",
    "from collections import namedtuple\n",
    "\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "initializer = mx.init.Xavier(factor_type=\"in\", magnitude=2.34)\n",
    "\n",
    "# Create arguments\n",
    "arg_names = cnn.list_arguments()\n",
    "input_shapes = {'data': DATA_SHAPE}\n",
    "shapey = cnn.infer_shape(**input_shapes)\n",
    "arg_shape, out_shape, aux_shape = shapey\n",
    "\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']:  # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "# Group symbols and create executor\n",
    "cnn_exec = cnn.bind(ctx=ctx,\n",
    "                    args=arg_arrays,\n",
    "                    args_grad=args_grad,\n",
    "                    grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']:  # input, output\n",
    "        continue\n",
    "    initializer(name, arg_dict[name])\n",
    "\n",
    "    param_blocks.append((i, arg_dict[name], args_grad[name], name))\n",
    "\n",
    "out_dict = dict(zip(cnn.list_outputs(), cnn_exec.outputs))\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "m = CNNModel(cnn_exec=cnn_exec,\n",
    "             symbol=cnn,\n",
    "             data=data, \n",
    "             label=label,\n",
    "             param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000\n",
      "Processed 20000\n",
      "Processed 30000\n",
      "Processed 40000\n",
      "Processed 50000\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent\n",
    "max_grad_norm=5.0\n",
    "optimizer = mx.optimizer.create(\n",
    "    'sgd',\n",
    "    learning_rate=LR,\n",
    "    momentum = MOM,\n",
    "    wd=WD\n",
    ")\n",
    "        \n",
    "updater = mx.optimizer.get_updater(optimizer)\n",
    "\n",
    "for iteration in range(EPOCHS):\n",
    "    \n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    \n",
    "    for batchX, batchY in manual_iterator(train_x, train_y, batch_size=BATCH_SIZE):\n",
    "\n",
    "        m.data[:] = batchX\n",
    "        m.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        m.cnn_exec.forward(is_train=True)\n",
    "        \n",
    "        # backward\n",
    "        m.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(m.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "        if num_total % (BATCH_SIZE*100) == 0:\n",
    "            print(\"Processed %d\" % num_total)\n",
    "            \n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in m.param_blocks:\n",
    "            grad /= BATCH_SIZE\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in m.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # decay learning rate\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "            \n",
    "    # end of training loop\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f' % (iteration, train_time, train_acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
