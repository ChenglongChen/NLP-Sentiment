{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet 69 characters:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', ',', ';', '.', '!', '?', ':', \"'\", '\"', '/', '\\\\', '|', '_', '@', '#', '$', '%', '^', '&', '*', '~', '`', '+', ' ', '=', '<', '>', '(', ')', '[', ']', '{', '}']\n"
     ]
    }
   ],
   "source": [
    "AZ_ACC = \"amazonsentimenik\"\n",
    "AZ_CONTAINER = \"textclassificationdatasets\"\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "print(\"Alphabet %d characters: \" % len(ALPHABET), ALPHABET)\n",
    "FEATURE_LEN = 1014\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    \n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    \n",
    "    if os.path.isfile(local_filename):\n",
    "        print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('saved data\\n')\n",
    "\n",
    "\n",
    "def load_data_frame(infile, batch_size=128, shuffle=True):\n",
    "\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "                  \n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'])\n",
    "           \n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    # store class as nparray\n",
    "    df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='int')\n",
    "    #print(Y_split[:30])\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "    \n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET)), columns=ALPHABET)\n",
    "    # Yield mini-batch amount of character vectors\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if ti % batch_size == 0:\n",
    "            # output\n",
    "            if ti > 0:\n",
    "                yield X_split, y_split[ti-batch_size:ti]\n",
    "            X_split = np.zeros([batch_size, FEATURE_LEN, len(ALPHABET)], dtype='int')\n",
    "            \n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti%batch_size][ci] = np.array(character_hash[ch])\n",
    "\n",
    "def example():\n",
    "    count = 0\n",
    "    for minibatch in load_data_frame('amazon_review_polarity_test.csv', batch_size=5, shuffle=True):\n",
    "        count += 1\n",
    "        print(minibatch[-1])\n",
    "        if count == 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file amazon_review_polarity_test.csv already exist in the current directory\n",
      "\n",
      "[1 1 0 1 1]\n",
      "[0 0 0 1 0]\n",
      "[1 0 0 1 0]\n",
      "[0 1 1 1 1]\n",
      "[0 0 1 1 0]\n",
      "[0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import mxnet as mx\n",
    "\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "def create_cnn_model(ctx,\n",
    "                     sentence_size, \n",
    "                     batch_size,\n",
    "                     vocab_size,\n",
    "                     num_label,\n",
    "                     filter_list,\n",
    "                     num_filter,\n",
    "                     dropout,\n",
    "                     initializer=mx.initializer.Uniform(0.1)):\n",
    "\n",
    "    \"\"\" \n",
    "    Create cnn_model with optional dropout and embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    input_x = mx.sym.Variable('data')\n",
    "    input_y = mx.sym.Variable('softmax_label')\n",
    "    \n",
    "                                   \n",
    "    # create convolutions and max pooling\n",
    "    pooled_outputs = []\n",
    "    \n",
    "    for i, filter_size in enumerate(filter_list):\n",
    "        \n",
    "        convi = mx.sym.Convolution(data=input_x,\n",
    "                                   kernel=(filter_size, vocab_size),\n",
    "                                   num_filter=num_filter)\n",
    "        \n",
    "        relui = mx.sym.Activation(data=convi, \n",
    "                                  act_type='relu')\n",
    "        \n",
    "        pooli = mx.sym.Pooling(data=relui,\n",
    "                               pool_type='max', \n",
    "                               kernel=(sentence_size - filter_size + 1, 1),\n",
    "                               stride=(1,1))\n",
    "        \n",
    "        pooled_outputs.append(pooli)\n",
    " \n",
    "    # combine all pooled outputs\n",
    "    total_filters = num_filter * len(filter_list)\n",
    "    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "    h_pool = mx.sym.Reshape(data=concat, target_shape=(batch_size, total_filters))\n",
    "\n",
    "    # dropout layer\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "\n",
    "    # fully connected\n",
    "    cls_weight = mx.sym.Variable('cls_weight')\n",
    "    cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "    fc = mx.sym.FullyConnected(data=h_drop,\n",
    "                               weight=cls_weight,\n",
    "                               bias=cls_bias,\n",
    "                               num_hidden=num_label)\n",
    "\n",
    "    # softmax output\n",
    "    cnn = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "    # get arguments\n",
    "    arg_names = cnn.list_arguments()\n",
    "\n",
    "    # shape\n",
    "    input_shapes = {}\n",
    "    input_shapes['data'] = (batch_size, sentence_size, vocab_size)\n",
    "\n",
    "    arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "    arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "    args_grad = {}\n",
    "    \n",
    "    for shape, name in zip(arg_shape, arg_names):\n",
    "        if name in ['softmax_label', 'data']: # input, output\n",
    "            continue\n",
    "        args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "    cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "    param_blocks = []\n",
    "    arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "    \n",
    "    for i, name in enumerate(arg_names):\n",
    "        if name in ['softmax_label', 'data']: # input, output\n",
    "            continue\n",
    "        initializer(name, arg_dict[name])\n",
    "\n",
    "        param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "    out_dict = dict(zip(cnn.list_outputs(), cnn_exec.outputs))\n",
    "\n",
    "    data = cnn_exec.arg_dict['data']\n",
    "    label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "    return CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)\n",
    "    \n",
    "    \n",
    "def train_cnn(model,\n",
    "              fun_data,\n",
    "              optimiser='rmsprop',\n",
    "              learning_rate = 0.0005,\n",
    "              epochs=200):\n",
    "    \n",
    "    m = model\n",
    "    opt = mx.optimizer.create(optimiser)\n",
    "    opt.lr = learning_rate\n",
    "    \n",
    "    updater = mx.optimizer.get_updater(opt)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        \n",
    "        for batchX, batchY in fun_data:\n",
    "\n",
    "            print(m.data[:].shape) # (128, 1014)\n",
    "            print(m.label[:].shape) # (128,)\n",
    "            \n",
    "            print(batchX.shape) # (128, 1014, 69)\n",
    "            print(batchY.shape) # (128,)\n",
    "            \n",
    "            m.data[:] = batchX\n",
    "            m.label[:] = batchY\n",
    "            \n",
    "            # forward\n",
    "            m.cnn_exec.forward(is_train_True)\n",
    "            \n",
    "            # backward\n",
    "            m.cnn_exec.backward()\n",
    "            \n",
    "            # evaluate on training\n",
    "            num_correct += sum(batchY == np.argmax(m.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "            num_total += len(batchY)\n",
    "\n",
    "        # end of training loop\n",
    "        train_acc = num_correct * 100 / float(num_total)\n",
    "        print(\"Iter [%d], Training Accuracy: %.3f\" % (epoch, train_acc) )\n",
    "\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #1. Create model\n",
    "    cnn_model = create_cnn_model(mx.cpu(),\n",
    "                                 sentence_size=FEATURE_LEN,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 vocab_size=len(ALPHABET),\n",
    "                                 num_label=2,\n",
    "                                 filter_list=[3, 4, 5],\n",
    "                                 num_filter=100,\n",
    "                                 dropout=0.5)\n",
    "                                 \n",
    "    #2. Train model       \n",
    "    train_cnn(model=cnn_model,\n",
    "              fun_data=load_data_frame('amazon_review_polarity_test.csv', BATCH_SIZE))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
