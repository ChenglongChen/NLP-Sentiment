{"nbformat_minor": 0, "cells": [{"source": "## Upload Review Data using AzureML\n\nCreate a batch file and execute:\n    \n```\ncd \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\"\nAzCopy /Source:C:\\_ilia_share\\amazon_prod_reviews_clean\\raw /Dest:https://ikcentralusstore.blob.core.windows.net/amazonrev /DestKey:dLR5lH2QN/ejGmyD61nQoh7Cc2DW8jIKhR5n5uvGu8+H3Qem4J0XzWG1/7XtBxmVlWr+y/GNRlwX4Km5YU68sg== /Pattern:\"aggressive_dedup.json\"\npause\n```", "cell_type": "markdown", "metadata": {}}, {"source": "## Load Review Data (from Blob)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "# paths\nblob = \"wasb://amazonrev@ikcentralusstore.blob.core.windows.net\"\njson_dta = blob + \"/aggressive_dedup.json\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Creating SparkContext as 'sc'\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1469092805367_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-ikclus.ftd4jbtqjxzuhd0uvhsmx0be3e.gx.internal.cloudapp.net:8088/proxy/application_1469092805367_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.15:30060/node/containerlogs/container_e02_1469092805367_0004_01_000001/spark\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "Creating HiveContext as 'sqlContext'\nSparkContext and HiveContext created. Executing user code ...\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# load data\njsonFile = sqlContext.read.json(json_dta)\njsonFile.registerTempTable(\"reviews\")\n\nprint(type(jsonFile))\njsonFile.show(5)\n\n# Note: also load the IMDB data at some point\n# ...", "outputs": [{"output_type": "stream", "name": "stdout", "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|      asin|helpful|overall|          reviewText| reviewTime|          reviewerID|   reviewerName|             summary|unixReviewTime|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\n|B003UYU16G| [0, 0]|    5.0|It is and does ex...|11 21, 2012|A00000262KYZUE4J5...| Steven N Elich|Does what it's su...|    1353456000|\n|B005FYPK9C| [0, 0]|    5.0|I was sketchy at ...| 01 8, 2013|A000008615DZQRRI9...|      mj waldon|           great buy|    1357603200|\n|B000VEBG9Y| [0, 0]|    3.0|Very mobile produ...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great product but...|    1395619200|\n|B001EJMS6K| [0, 0]|    4.0|Easy to use a mob...|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great inexpensive...|    1395619200|\n|B003XJCNVO| [0, 0]|    4.0|Love this feeder....|03 24, 2014|A00000922W28P2OCH...|Gabriel Merrill|Great feeder. Wou...|    1395619200|\n+----------+-------+-------+--------------------+-----------+--------------------+---------------+--------------------+--------------+\nonly showing top 5 rows"}], "metadata": {"collapsed": false}}, {"source": "## Examine some of the reviews", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -o topn\nSELECT overall, reviewText\nFROM reviews\nLIMIT 10", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -o revs\nSELECT overall, COUNT(overall) as freq\nFROM reviews\nGROUP BY overall\nORDER by -freq", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%sql -o avgrev\nSELECT reviewText\nFROM reviews\nWHERE overall = '3'\nLIMIT 10", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "# Create a dataframe of our reviews\n# To analyse class imbalance\n\nreviews =  sqlContext.sql(\"SELECT \" + \n                          \"CASE WHEN overall < 3 THEN 'low' \" +\n                          \"WHEN overall > 3 THEN 'high' ELSE 'mid' END as label, \" + \n                          \"reviewText as sentences \" + \n                          \"FROM reviews\")\n# Tally\n#tally = reviews.groupBy(\"label\").count()\n#tally.show()\n\"\"\"\nmid| 7,039,272\nlow|10,963,811\nhigh|64,453,794\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'\\nmid| 7,039,272\\nlow|10,963,811\\nhigh|64,453,794\\n'"}], "metadata": {"collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "# Let's look at some reviews to see how clean they are\n# there seems to be lots of html formatting\nfor c,r in enumerate(reviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1. It is and does exactly what the description said it would be and would do. Couldn't be happier with it.\n2. I was sketchy at first about these but once you wear them for a couple hours they break in they fit good on my board an have little wear from skating in them. They are a little heavy but won't get eaten up as bad by your grip tape like poser dc shoes.\n3. Very mobile product. Efficient. Easy to use; however product needs a varmint guard. Critters are able to gorge themselves without a guard.\n4. Easy to use a mobile. If you're taller than 4ft, be ready to tuck your legs behind you as you hang and pull.\n5. Love this feeder. Heavy duty & capacity. Best feature is the large varmint guard. Definitely use a small lock or securing device on the battery housing latch. I gave 4 stars because several bolts were missing. Check contents b4 beginning.\n6. Solid, stable mount. Holds iPhone with phone protector well. I have not however used the dash mount part of this product (only windshield).\n7. I bought this pepper because I wanted a lot of cayenne powder I mean a lot. I drink shots of this powder daily and I do like it but I'm not sure if it's just me but it does not seem to strong I sweat for a minute but I feel like it could be stronger I even touched my eyes to see if it would hurt still not feeling much pain. Im one in 7 billion so do forget my review since I'm more hardcore.\n8. Beautiful photos/film with wonderful music.  Giriodi lets you know where you are with on screen notes.  It helps to keep your heart in Colorado when you can't be there.\n9. My idea of Colorado is &#34;Mountains&#34;.  Colorado Landscapes seemed to focus on the flatlands and foothills.  It gave you no idea where you were  - it could have used a narrator or notes on-screen.\n10. No matter what we did the bills just kept jamming in the machine. The bill counts were not consistent. We returned the product because it did not work correctly."}], "metadata": {"collapsed": false}}, {"execution_count": 6, "cell_type": "code", "source": "# Some very basic cleaning\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import StringType, DoubleType \nfrom bs4 import BeautifulSoup\n\ndef cleanerHTML(line):\n    # html formatting\n    html_clean = BeautifulSoup(line, \"lxml\").get_text().lower()\n    # remove any double spaces, line-breaks, etc.\n    return \" \".join(html_clean.split())\n\ndef labelForResults(s):\n    # string label to numeric\n    if s == 'low':\n        return 0.0\n    elif s == 'high':\n        return 1.0\n    else:\n        return -1.0\n        \ncleaner = UserDefinedFunction(cleanerHTML, StringType())\nlabel = UserDefinedFunction(labelForResults, DoubleType())\n\ncleanedReviews = reviews.select(reviews.label,\n                                label(reviews.label).alias('sentiment'), \n                                cleaner(reviews.sentences).alias('sentences'))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "source": "# A bit cleaner ...\nreviews = cleanedReviews\nfor c,r in enumerate(reviews.take(10)):\n    print(\"%d. %s\" % (c+1,r['sentences']))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1. it is and does exactly what the description said it would be and would do. couldn't be happier with it.\n2. i was sketchy at first about these but once you wear them for a couple hours they break in they fit good on my board an have little wear from skating in them. they are a little heavy but won't get eaten up as bad by your grip tape like poser dc shoes.\n3. very mobile product. efficient. easy to use; however product needs a varmint guard. critters are able to gorge themselves without a guard.\n4. easy to use a mobile. if you're taller than 4ft, be ready to tuck your legs behind you as you hang and pull.\n5. love this feeder. heavy duty & capacity. best feature is the large varmint guard. definitely use a small lock or securing device on the battery housing latch. i gave 4 stars because several bolts were missing. check contents b4 beginning.\n6. solid, stable mount. holds iphone with phone protector well. i have not however used the dash mount part of this product (only windshield).\n7. i bought this pepper because i wanted a lot of cayenne powder i mean a lot. i drink shots of this powder daily and i do like it but i'm not sure if it's just me but it does not seem to strong i sweat for a minute but i feel like it could be stronger i even touched my eyes to see if it would hurt still not feeling much pain. im one in 7 billion so do forget my review since i'm more hardcore.\n8. beautiful photos/film with wonderful music. giriodi lets you know where you are with on screen notes. it helps to keep your heart in colorado when you can't be there.\n9. my idea of colorado is \"mountains\". colorado landscapes seemed to focus on the flatlands and foothills. it gave you no idea where you were - it could have used a narrator or notes on-screen.\n10. no matter what we did the bills just kept jamming in the machine. the bill counts were not consistent. we returned the product because it did not work correctly."}], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": "reviews.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+---------+--------------------+\n|label|sentiment|           sentences|\n+-----+---------+--------------------+\n| high|      1.0|it is and does ex...|\n| high|      1.0|i was sketchy at ...|\n|  mid|     -1.0|very mobile produ...|\n| high|      1.0|easy to use a mob...|\n| high|      1.0|love this feeder....|\n| high|      1.0|solid, stable mou...|\n| high|      1.0|i bought this pep...|\n| high|      1.0|beautiful photos/...|\n|  low|      0.0|my idea of colora...|\n|  low|      0.0|no matter what we...|\n|  low|      0.0|i do not suggest ...|\n|  low|      0.0|useless - all you...|\n| high|      1.0|this book is real...|\n| high|      1.0|it is not a stick...|\n| high|      1.0|love the size and...|\n| high|      1.0|its very colorful...|\n| high|      1.0|the condition of ...|\n| high|      1.0|only negative. th...|\n| high|      1.0|bought this for m...|\n| high|      1.0|this book is a gr...|\n+-----+---------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 9, "cell_type": "code", "source": "# Equalise classes for this example\n# 10 mill pos and 10 mill neg\nnum_reviews = 1000 # sample\nneg_rev = reviews.filter(\"sentiment = 0.0\").limit(num_reviews)\npos_rev = reviews.filter(\"sentiment = 1.0\").limit(num_reviews)\n\n# split intro training and test (50%, 50%)\ntrainingData, testData = neg_rev.unionAll(pos_rev).randomSplit([0.5, 0.5])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 10, "cell_type": "code", "source": "#print(neg_rev.count(), pos_rev.count())", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "#trainingData.show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "#testData.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## TFIDF", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "# Some sample data to use\n# to save time ...\n\nsampleData = sqlContext.createDataFrame([\n  (0.0, \"Hi, I heard about Spark\"),\n  (0.0, \"I couldn't wish Java was any different\"),\n  (1.0, \"Logistic regression models are super boring\")], \n                        [\"sentiment\", \"sentences\"])\nsampleData.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---------+--------------------+\n|sentiment|           sentences|\n+---------+--------------------+\n|      0.0|Hi, I heard about...|\n|      0.0|I couldn't wish J...|\n|      1.0|Logistic regressi...|\n+---------+--------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 119, "cell_type": "code", "source": "# Example to get ngrams-range\nfrom itertools import chain\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.ml.feature import NGram\n\nclass nGramRanger:\n    \n    \"\"\"\n    \n    Wrapper around pyspark.ml.feature.Ngram to call that in a loop and produce a \n    range of ngrams. \n    \n    ngramRange = (1,3) creates Ngram(1) and Ngram(2) and Ngram(3)\n    \n    \"\"\"\n    \n    def __init__(self, ngramRange, inputCol, outputCol = \"ngrams\"):\n        # ngramRange = (1,3)\n        self.ngramRange = ngramRange\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n\n    def transform(self, inDf):\n        _orig = inDf\n        def concat(type):\n            def concat_(*args):\n                return list(chain(*args))\n            return udf(concat_, ArrayType(type))\n        \n        # Create columns for n-grams\n        for ngr in range(self.ngramRange[0],self.ngramRange[-1]+1):\n            ngram = NGram(inputCol=self.inputCol , n=ngr, outputCol=\"%sgram\" % ngr)\n            inDf = ngram.transform(inDf)        \n        # Combine\n        concat_string_arrays = concat(StringType())\n        ###!!!!! FIx this at some point as 3 grams manually !!!!!\n        outDta = _orig.join(inDf.select(concat_string_arrays(col(\"1gram\"),\n                                                            col(\"2gram\"),\n                                                            col(\"3gram\")).alias(self.outputCol)))\n        # Return dataframe\n        return outDta\n\n\"\"\"\ntokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\ntokenisedData = tokenizer.transform(sampleData)\n\nstopremover = StopWordsRemover(inputCol=\"words\", outputCol=\"wordsFiltered\")\nstoppedData = stopremover.transform(tokenisedData)\n\nngrammer = nGramRanger(ngramRange = (1,3), inputCol = \"wordsFiltered\", outputCol = \"ngrams\")\noutput_test = ngrammer.transform(stoppedData)\n\nfor rw in output_test.take(5):\n    print(rw)\n\"\"\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Row(sentiment=0.0, sentences=u'Hi, I heard about Spark', words=[u'hi,', u'i', u'heard', u'about', u'spark'], wordsFiltered=[u'hi,', u'heard', u'spark'], ngrams=[u'hi,', u'heard', u'spark', u'hi, heard', u'heard spark', u'hi, heard spark'])\nRow(sentiment=0.0, sentences=u'Hi, I heard about Spark', words=[u'hi,', u'i', u'heard', u'about', u'spark'], wordsFiltered=[u'hi,', u'heard', u'spark'], ngrams=[u\"couldn't\", u'wish', u'java', u'different', u\"couldn't wish\", u'wish java', u'java different', u\"couldn't wish java\", u'wish java different'])\nRow(sentiment=0.0, sentences=u'Hi, I heard about Spark', words=[u'hi,', u'i', u'heard', u'about', u'spark'], wordsFiltered=[u'hi,', u'heard', u'spark'], ngrams=[u'logistic', u'regression', u'models', u'super', u'boring', u'logistic regression', u'regression models', u'models super', u'super boring', u'logistic regression models', u'regression models super', u'models super boring'])\nRow(sentiment=0.0, sentences=u\"I couldn't wish Java was any different\", words=[u'i', u\"couldn't\", u'wish', u'java', u'was', u'any', u'different'], wordsFiltered=[u\"couldn't\", u'wish', u'java', u'different'], ngrams=[u'hi,', u'heard', u'spark', u'hi, heard', u'heard spark', u'hi, heard spark'])\nRow(sentiment=0.0, sentences=u\"I couldn't wish Java was any different\", words=[u'i', u\"couldn't\", u'wish', u'java', u'was', u'any', u'different'], wordsFiltered=[u\"couldn't\", u'wish', u'java', u'different'], ngrams=[u\"couldn't\", u'wish', u'java', u'different', u\"couldn't wish\", u'wish java', u'java different', u\"couldn't wish java\", u'wish java different'])"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Pipeline for feature selection and classification\n# Using https://spark.apache.org/docs/1.5.2/ml-features.html\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, NGram\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n\nnumfeat = 40000\nngram_range = (1,3)\n\n# 1. Feature-extraction\ntokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\nngrammer = nGramRanger(ngramRange = ngram_range, inputCol = \"words\", outputCol = \"ngrams\")\nhashingtf  = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=numfeat)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\n# Train\ntokenized_train = tokenizer.transform(trainingData)\nngrammed_train = ngrammer.transform(tokenized_train)\nhashed_train = hashingtf.transform(ngrammed_train)\nidfModel = idf.fit(hashed_train)\nidf_train = idfModel.transform(hashed_train)\n\n# Test\ntokenized_test = tokenizer.transform(testData)\nngrammed_test = ngrammer.transform(tokenized_test)\nhashed_test = hashingtf.transform(ngrammed_test)\nidf_test = idfModel.transform(hashed_test)\n\n# 2. Classifier\nclassi = LogisticRegression(labelCol=\"sentiment\", featuresCol=\"features\")\n\n# Train\ntfidfModel = classi.fit(idf_train)\n\n# Predict\npred = tfidfModel.transform(idf_test)\n\n# 3. Examine\nnumSuccesses = pred.where(\"\"\"(prediction = 0 AND label = 'low') OR (prediction = 1 AND label = 'high')\"\"\").count()\nnumInspections = pred.count()\nacc = (float(numSuccesses) / float(numInspections)) * 100\nprint(\"%.2f success rate\" % acc) # 51.41 success rate", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "widgets": {"state": {"311e1fefb0374b22a67c5617c78b0175": {"views": []}, "86300076eee8481b8120156b1ae3be48": {"views": []}, "2cfdac2d462a49d9bf9dc855e01a4558": {"views": []}, "b6f39c51ada24cf391b361d860c006a5": {"views": []}, "db6e35caff874d6095af34404a346697": {"views": []}, "8f2d6dc7ed8741aebf3896ddaf43f0ec": {"views": []}, "1b6ae16673e347d1bde18dc2c2bd489c": {"views": []}, "a396fd8abbf34e968d8f1c9cbde0dd95": {"views": []}, "ecc8037a720f4c0e9849cbe0848a8e8d": {"views": []}, "f4e654e761794cd394c7052753ac7340": {"views": []}, "a99cdbb2fe894aa984709c41d9e7b42a": {"views": []}, "898e69d1fe614ca2997e52490d88defb": {"views": []}, "900e5cf4e561445c8df4e7b8b82e4cc6": {"views": []}, "248216c110384d3091265c0e7861481e": {"views": []}, "21fd7c87a6b84bf6a59d345c7f75e3bb": {"views": []}, "3a80693339954db4884246c15ad51c6b": {"views": []}, "15cc4a8b1d6741aeb535c2a35913bdbd": {"views": []}, "408092ad3bd5470e82bebcd941df5183": {"views": []}, "859f3199a45a443d8a9dd55001a66576": {"views": []}, "1cfa52d618cf4eb297cf3ec2ba6ad690": {"views": [{"cell_index": 7}]}, "cca8590e2287453692ee8500609040cb": {"views": []}, "b20c9a86372c4f4eaf4e418df9fe0ef5": {"views": []}, "39ce038fa5a748459e26dfe1705f0c18": {"views": []}, "1393a426fdaf4693ae1e15b046427cd6": {"views": []}, "ccad85bf3d404e2db5c70e52e72a1a41": {"views": []}, "af8b725f5e71442297dbf9b422f5c507": {"views": []}, "118e44f01e4d4345a9075838e3a9b2c3": {"views": []}, "14a4372b8dd94fdb8cb1b74ce3f551b1": {"views": []}, "db39391d5e2540038b17c260cd196a53": {"views": []}, "cd20c857a2d2437cab200a47ef43d28d": {"views": []}, "ab00194233084f7aa46781d51facbb41": {"views": []}, "cd512a3e08f94a0a8ec6e814f4a46a5d": {"views": []}, "b08cf9747921430ea4c2cc1eaf92531a": {"views": []}, "b187e86f198d4a2fb4548cd4ebb5a3eb": {"views": []}, "2a47a95ea456400694db8959898dfbfa": {"views": []}, "ff1f80e2326847219a7754981e9014c0": {"views": []}, "10b1fae3524c4d4b8bbf59e87b6aaa6a": {"views": []}, "e6e37c06b29e4c16aa54ea5541397d5c": {"views": []}, "f51f169bd4644ca99511343f75ef0f5a": {"views": []}, "5ec4a2fe9f7c460b90f17e051a7c7124": {"views": []}, "06e3861a73de48e6839cfe0a538bee72": {"views": []}, "4fc9d515adfa409797b0844ef0d282b1": {"views": []}, "99d4eb4f909444979b3e7baa4228c543": {"views": []}, "5e28777210414136a7a704705b5679bb": {"views": []}, "3929243187cc4d2f94bf4779d1476291": {"views": []}, "459fef39e7f14bb7b035522f37de8ed5": {"views": []}, "d1a0d633fb994af4a7b8a108c45b415c": {"views": []}, "e69667278eae496288b936d73c6d78af": {"views": []}, "598f3f051029447992601b7f7ace1383": {"views": []}, "d980cf7258af40b49ddab4255422ef80": {"views": []}, "22037676266f4833a70f4d48f639f716": {"views": []}, "6c995e8cf56946839982a994eedc2a91": {"views": []}, "462a4a58fc4b4d03827dc3968e6ab865": {"views": []}, "eb543c18498143978f9dca068e25c7c7": {"views": []}, "d348b5f980cd4d55af6e3a474dc14d77": {"views": [{"cell_index": 5}]}, "44115dfc2a7e42eabb90db3390b02937": {"views": []}, "af4f03a89a4841a2aef9b01c31569073": {"views": []}, "4cd76ba683b74f5e947cdf7a23c57085": {"views": []}, "7e20ed182e1a415fbf224c38237d2947": {"views": []}, "6ee6a6476e5c4c9581f2e0a15f857726": {"views": []}, "0cf95e486f81460eba58d549123ec563": {"views": []}, "857a36aee0314d3b9ecb1a1c62aed4a4": {"views": []}, "ec18ce1c03864f2abc1a48fbfd9e3127": {"views": []}, "a248d53f418948289f97d8560374df1b": {"views": []}, "ce271d640b714c97945e0a8d1cd314f7": {"views": [{"cell_index": 5}]}, "ebd6e827d6d34da5ba3ef14758a75dfb": {"views": []}, "d90a462aa6ad46db866f46c404dbb361": {"views": []}, "854ffd7d256d4da3800295a3984c1834": {"views": []}, "50a8765f355440149b067b74f9c86e69": {"views": []}, "b8ecf011e79f441fa1a2a6b5b44a4e3a": {"views": []}, "c041cc5bc0b94573bb65a03eaf0ee261": {"views": []}, "f06d3a6e901c400d9f35038f2697e180": {"views": [{"cell_index": 6}]}, "47b14d2d6ac948b1bae401131689db10": {"views": []}, "d77ea53bea284975a1f462b642326407": {"views": []}, "c7a4c738c9e849e2a939aa554bb8567e": {"views": []}, "6d6984f0ad2446c995748619add59c7f": {"views": []}, "766f8b292dd64bcca65105746fa16159": {"views": []}, "cb839beef8294f5a818b22f15b7f4d57": {"views": []}, "5f17fcc1d1a64b318f7441b9795bc554": {"views": []}, "998fa0f0f5444159bb769d2b7d3a22ff": {"views": []}, "14cd56af67864e11baa8e88e721ba6ae": {"views": []}, "425a4240ca0442e084bf0030a1a38bc1": {"views": []}, "d74a3a1216f24fdcaaf70d805f395d6a": {"views": []}, "4c5a4a0485d442d1afd64e2d50588c32": {"views": []}, "b9e1203a411c400ea9cb042d63eeb544": {"views": []}, "bc5971a539b9448fadb4b698fea34f0f": {"views": [{"cell_index": 7}]}, "effbdfbc6239446e869d52551e7b8259": {"views": []}, "32f10c1106114d498311f147245a1aa2": {"views": []}, "31a3da0c91f14952b93533d6a53bb94a": {"views": []}, "1bc3183a8ce54417902261d504dbd1f1": {"views": []}, "a5899cdb248c4ce8ba9d9c0d9b1c52fb": {"views": []}, "9870055d5d5f45f99c27447e5383e6d1": {"views": []}, "5538d859063a443aa6357cc6b9a678a5": {"views": []}, "2d0c9eecc7e4418cbd04598ffc023f92": {"views": []}, "55be7ec92a3c4c198837fa8a92024ceb": {"views": []}, "b5132a5a073247788014346463ae404f": {"views": []}, "9368f319d2fb4c408b6ae6350a8799b8": {"views": []}, "5c7ed0d7e3e04455890dbdd9e3860017": {"views": []}, "953fd8da5b9748878aed1008328ebf74": {"views": []}, "8845aac67ade463dadbd083e710ca84a": {"views": []}, "c2bdaf86745f445c90fee6c9867ec2af": {"views": []}, "b1d0342b55ef422a85b3183f62f1d740": {"views": []}, "55defd671e0d461f9a92556949f5e920": {"views": []}, "bf8e8bad484d4114a9dad8bef520b7ac": {"views": []}, "86378ef9578a4d64840e2fb433a25bc8": {"views": []}, "dfb49dbf4bfe4bf38fa9f65344ecd8e2": {"views": []}, "4f3b74a8715741888edd05a33a84b4d9": {"views": []}, "f093bfbfbae448658290ca977e0ef5cb": {"views": []}, "ff98fb41d4b3424f8904081d379d29e9": {"views": []}, "2b0e27c2beff4983bcc4474a5f647094": {"views": []}, "61f484dc93904007a8e51938cbb70b61": {"views": []}, "5fbc7b40dfb4465ca1ca9fc778e643fa": {"views": []}, "c4485ce569fc45bb9eacf5bffbc96021": {"views": []}, "33f28392694a40578a063a59fdf45f31": {"views": []}, "595449e5c5334b5092a134444088a633": {"views": []}, "e3d0ac9f2b954c76abd5f5d1a2b25fec": {"views": []}, "24e4431cea1946598be8aff890de0af3": {"views": []}, "a1988b98ffe340cb9dc68721c27b9fcb": {"views": [{"cell_index": 6}]}, "98573d53ca44497b9e46e03d4974fdea": {"views": []}, "4d3d4431f9aa454482fb9fb6ea9852a6": {"views": []}, "e2e06a8d196a4933a52acc671aa6bcc4": {"views": []}, "4fb0b0aef6d546bd9797814d34acf127": {"views": []}, "c6f155483681453a8566c3f92a9fb26b": {"views": []}, "db346fcbf3884619aa2bbbcbeb2ae661": {"views": []}}, "version": "1.1.2"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"name": "python"}}}}